"""
Causal Graph Neural Network Trainer (ç®€æ´ä¼˜é›…ç‰ˆ)
ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹ï¼šé¢„è®­ç»ƒ + ä¸¤é˜¶æ®µå› æœå­¦ä¹ 

æ ¸å¿ƒç‰¹ç‚¹ï¼š
1. ç®€æ´çš„ç¨€ç–åº¦æ§åˆ¶ - ç›´æ¥æƒ©ç½šå› æœç‰¹å¾æ•°é‡
2. æ¸è¿›å¼å¢å¼ºç­–ç•¥ - è®©æ¨¡å‹è‡ªç„¶æ‰¾åˆ°æœ€ä¼˜å¹³è¡¡ç‚¹
3. æ¸…æ™°çš„ä»£ç ç»“æ„ - æ˜“è¯»æ˜“ç»´æŠ¤
"""

import os
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR
from typing import Dict, Any, Optional
import numpy as np

from utils.checkpoint import CheckpointManager
from models.causal_net import CausalNet
from models.causal_mask import CausalMask
from data.large_graph_builder import LargeGraphBuilder
from utils.metrics import compute_binary_metrics

logger = logging.getLogger(__name__)


class CausalTrainer:
    """
    å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
    
    è®­ç»ƒæµç¨‹ï¼š
    1. é¢„è®­ç»ƒé˜¶æ®µ (40 epochs)ï¼šæ•´ä½“å›¾é¢„æµ‹ï¼Œå»ºç«‹åŸºç¡€ç‰¹å¾è¡¨ç¤º
    2. é˜¶æ®µ1 (40 epochs)ï¼šå­¦ä¹ å› æœæ©ç ï¼ˆå†…åœ¨å­å›¾ + è™šå‡å­å›¾ï¼‰
    3. é˜¶æ®µ2 (60 epochs)ï¼šå› æœæµ‹è¯•ä¸é²æ£’æ€§å¢å¼ºï¼ˆèåˆå›¾å¹²æ‰°ï¼‰
    
    æ ¸å¿ƒç»„ä»¶ï¼š
    - DenseNetï¼šå†»ç»“çš„ç‰¹å¾æå–å™¨
    - CausalMaskï¼šå¯å­¦ä¹ çš„å› æœæ©ç ï¼ˆèŠ‚ç‚¹ + è¾¹ï¼‰
    - CausalNetï¼šå›¾ç¥ç»ç½‘ç»œåˆ†ç±»å™¨
    - LargeGraphBuilderï¼šå¯¹æ¯”å­¦ä¹ çš„å¤§å›¾æ„å»ºå™¨
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        fold: int,
        densenet_model: nn.Module,
        edge_prior_mask: np.ndarray,
        checkpoint_manager: CheckpointManager,
        work_dir: str,
        device: str = 'cuda',
        rank: int = 0
    ):
        self.config = config
        self.fold = fold
        self.densenet_model = densenet_model.to(device)
        self.edge_prior_mask = torch.FloatTensor(edge_prior_mask).to(device)
        self.checkpoint_manager = checkpoint_manager
        self.work_dir = work_dir
        self.device = device
        self.rank = rank
        
        # å†»ç»“DenseNetç‰¹å¾æå–å™¨
        for param in self.densenet_model.parameters():
            param.requires_grad = False
        self.densenet_model.eval()
        
        # æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.model = None
        self.mask = None
        self.optimizer = None
        self.optimizer_mask = None
        self.optimizer_pretrain = None
        self.lr_scheduler = None
        self.lr_scheduler_mask = None
        self.scheduler_pretrain = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.pretrain_best_val_acc = 0.0
        self.pretrain_best_model_state = None
        self.pretrain_best_epoch = -1
        self.best_val_acc = 0.0
        self.best_test_acc = 0.0
        self.best_epoch = -1
        self.best_model_state = None
        
        # è®°å½•å’Œç›‘æ§
        self.epoch_results = {}
        self.current_mask_sums = {}
        
        # å¤§å›¾æ„å»ºå™¨ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        self.large_graph_builder = LargeGraphBuilder(
            num_neg_samples=config['large_graph']['num_neg_samples'],
            sampling_strategy=config['large_graph']['sampling_strategy'],
            random_seed=config['seed']
        )
        
        # ç¼“å­˜å…¨å±€æ•°æ®ï¼ˆç”¨äºè´Ÿæ ·æœ¬é‡‡æ ·ï¼‰
        self.all_data = None
        self.all_labels = None
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(reduction='none')
        self.lambda_l1 = config['train']['loss_weights']['lambda_l1']
        
        logger.info(f"âœ… Trainer initialized for Fold {fold}")
    
    # ============================================================================
    # æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆå§‹åŒ–
    # ============================================================================
    
    def _build_models(self):
        """æ„å»ºå› æœå›¾ç¥ç»ç½‘ç»œå’Œæ©ç æ¨¡å‹"""
        # ä¸»GNNæ¨¡å‹
        self.model = CausalNet(
            num_class=2,
            feature_dim=self.densenet_model.feature_dim,
            hidden1=self.config['model']['args']['hidden1'],
            hidden2=self.config['model']['args']['hidden2'],
            kernels=self.config['model']['args'].get('kernels', [2]),
            num_patches=self.edge_prior_mask.shape[0],
            num_neg_samples=self.config['large_graph']['num_neg_samples']
        ).to(self.device)
        
        # å› æœæ©ç æ¨¡å‹ï¼ˆç®€æ´ç‰ˆ - æ— ç›®æ ‡ç¨€ç–åº¦ï¼‰
        self.mask = CausalMask(
            num_patches=self.edge_prior_mask.shape[0], 
            edge_matrix=self.edge_prior_mask,    
            gumble_tau=self.config['misc']['gumble_tau']
        ).to(self.device)
        
        # å¤šGPUå¹¶è¡Œ
        if torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            self.mask = nn.DataParallel(self.mask)
        
        logger.info("âœ… Models built")
    
    def _setup_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # é¢„è®­ç»ƒé˜¶æ®µï¼šAdamä¼˜åŒ–å™¨ + Cosineé€€ç«
        self.optimizer_pretrain = optim.Adam(
            self.model.parameters(),
            lr=self.config['densenet']['pretrain']['learning_rate'],
            weight_decay=self.config['densenet']['pretrain']['weight_decay']
        )
        self.scheduler_pretrain = CosineAnnealingLR(
            self.optimizer_pretrain,
            T_max=self.config['train']['pre_epoch'],
            eta_min=1e-5
        )

        # ä¸»è®­ç»ƒé˜¶æ®µï¼šSGD + ReduceLROnPlateau
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=self.config['train']['base_lr'],
            momentum=0.9,
            nesterov=True,
            weight_decay=self.config['train']['weight_decay']
        )
        self.optimizer_mask = optim.SGD(
            self.mask.parameters(),
            lr=self.config['train']['base_lr_mask'],
            momentum=0.9,
            nesterov=True,
            weight_decay=self.config['train']['weight_decay']
        )

        self.lr_scheduler = ReduceLROnPlateau(
            self.optimizer,
            verbose=(self.rank == 0),
            patience=self.config['train']['stepsize'],
            factor=self.config['train']['gamma']
        )
        self.lr_scheduler_mask = ReduceLROnPlateau(
            self.optimizer_mask,
            verbose=(self.rank == 0),
            patience=self.config['train']['stepsize'],
            factor=self.config['train']['gamma']
        )

        logger.info("âœ… Optimizers configured")
    
    # ============================================================================
    # ä¸»è®­ç»ƒå…¥å£
    # ============================================================================
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader
    ) -> Dict[str, float]:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Returns:
            æœ€ç»ˆè¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ Training Start - Fold {self.fold}")
        logger.info(f"{'='*80}\n")
        
        # é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º
        if self.all_data is None:
            logger.info("é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º...")
            dataset = train_loader.dataset
            if hasattr(dataset, 'dataset'):
                dataset = dataset.dataset
            self.all_data = np.array(dataset.all_patches)
            self.all_labels = np.array(dataset.labels)
            logger.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆ: {self.all_data.shape}")        
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
        self._build_models()
        self._setup_optimizers()
        
        # é˜¶æ®µ1: é¢„è®­ç»ƒ (æ•´ä½“å›¾é¢„æµ‹)
        pre_epochs = self.config['train']['pre_epoch']
        if pre_epochs > 0:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“š Phase 1: Pre-training (Whole Graph Prediction)")
            logger.info("="*80)
            self._pretrain_phase(train_loader, val_loader, test_loader, pre_epochs)
        
        # é˜¶æ®µ2+3: ä¸»è®­ç»ƒ (å› æœæ©ç å­¦ä¹  + é²æ£’æ€§å¢å¼º)
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ Phase 2+3: Causal Learning (Mask + GNN Co-training)")
        logger.info("="*80)
        self._main_training(train_loader, val_loader, test_loader)
        
        # æœ€ç»ˆè¯„ä¼°
        final_results = self._final_evaluation(test_loader)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… Training Completed - Fold {self.fold}")
        logger.info(f"   Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"   Best Test Acc: {self.best_test_acc:.4f}")
        logger.info(f"{'='*80}\n")
        
        return final_results
    
    # ============================================================================
    # é¢„è®­ç»ƒé˜¶æ®µ (æ•´ä½“å›¾é¢„æµ‹)
    # ============================================================================
    
    def _pretrain_phase(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        num_epochs: int
    ):
        """é¢„è®­ç»ƒé˜¶æ®µï¼šå­¦ä¹ æ•´ä½“å›¾çš„é¢„æµ‹èƒ½åŠ›"""
        for epoch in range(num_epochs):
            if self.rank == 0:
                self.epoch_results[epoch] = {}

            # è®­ç»ƒ
            self._train_pretrain_epoch(epoch, train_loader)

            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_pretrain_epoch(epoch, val_loader, 'val')
                    self._eval_pretrain_epoch(epoch, test_loader, 'test')

                self._print_pretrain_summary(epoch)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler_pretrain.step()

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.rank == 0 and self.pretrain_best_model_state:
            logger.info(f"âœ… Loading best pretrain model (Epoch {self.pretrain_best_epoch+1})")
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(self.pretrain_best_model_state)
            else:
                self.model.load_state_dict(self.pretrain_best_model_state)
    
    def _train_pretrain_epoch(self, epoch: int, train_loader: DataLoader):
        """é¢„è®­ç»ƒçš„å•ä¸ªepoch"""
        self.model.train()
        
        losses = []
        accuracies = []
        
        for data, _, label in train_loader:
            self.global_step += 1
            
            data = data.to(self.device)
            label = label.to(self.device)
            
            # æå–ç‰¹å¾
            x_features = self._extract_features(data)
            
            # æ•´ä½“é¢„æµ‹
            outputs = self.model.module.prediction_whole(x_features, self.edge_prior_mask) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_prior_mask)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(outputs, label).mean()
            l1_loss = self._compute_l1_regularization()
            loss_total = loss + l1_loss
            
            # åå‘ä¼ æ’­
            self.optimizer_pretrain.zero_grad()
            loss_total.backward()
            self.optimizer_pretrain.step()
            
            # è®°å½•
            if self.rank == 0:
                losses.append(loss.item())
                _, predicted = torch.max(outputs, 1)
                acc = (predicted == label).float().mean().item()
                accuracies.append(acc)
        
        # ä¿å­˜ç»“æœ
        if self.rank == 0:
            self.epoch_results[epoch]['train'] = {
                'loss_all': np.mean(losses),
                'acc_Intrinsic': np.mean(accuracies)
            }
    
    def _eval_pretrain_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        """é¢„è®­ç»ƒçš„è¯„ä¼°"""
        self.model.eval()

        all_outputs = []
        all_labels = []

        for data, _, label in data_loader:
            data = data.to(self.device)
            label = label.to(self.device)

            x_features = self._extract_features(data)
            outputs = self.model.module.prediction_whole(x_features, self.edge_prior_mask) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_prior_mask)

            all_outputs.append(outputs)
            all_labels.append(label)

        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)

        metrics = compute_binary_metrics(all_outputs, all_labels)
        self.epoch_results[epoch][phase] = metrics

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if phase == 'val' and metrics['accuracy'] > self.pretrain_best_val_acc:
            self.pretrain_best_val_acc = metrics['accuracy']
            self.pretrain_best_epoch = epoch

            if isinstance(self.model, nn.DataParallel):
                self.pretrain_best_model_state = self.model.module.state_dict()
            else:
                self.pretrain_best_model_state = self.model.state_dict()

            logger.info(f"ğŸ’ New Best Pretrain - Val Acc: {metrics['accuracy']:.4f}, AUC: {metrics['auc']:.4f}")
    
    # ============================================================================
    # ä¸»è®­ç»ƒé˜¶æ®µ (å› æœæ©ç å­¦ä¹ )
    # ============================================================================
    
    def _main_training(self, train_loader, val_loader, test_loader):
        """ä¸»è®­ç»ƒé˜¶æ®µï¼šé˜¶æ®µ1ï¼ˆå› æœåˆ†ç¦»ï¼‰ + é˜¶æ®µ2ï¼ˆé²æ£’æ€§å¢å¼ºï¼‰"""
        start_epoch = self.config['train']['pre_epoch']
        num_epochs = self.config['train']['num_epoch']
        stage_transition = self.config['train']['stage_transition_epoch']

        # é˜¶æ®µ1çš„æœ€ä½³æ¨¡å‹è¿½è¸ª
        stage1_best_val_acc = 0.0
        stage1_best_model_state = None
        stage1_best_epoch = -1

        for epoch in range(start_epoch, num_epochs):
            is_stage1 = epoch < stage_transition

            if self.rank == 0:
                self.epoch_results[epoch] = {}

            # è®­ç»ƒ
            self._train_main_epoch(epoch, train_loader, is_stage1)

            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_main_epoch(epoch, val_loader, 'val')
                    self._eval_main_epoch(epoch, test_loader, 'test')

                # é˜¶æ®µ1ï¼šä¿å­˜é˜¶æ®µ1æœ€ä½³æ¨¡å‹
                if is_stage1:
                    val_acc = self.epoch_results[epoch]['val']['accuracy']
                    if val_acc > stage1_best_val_acc:
                        stage1_best_val_acc = val_acc
                        stage1_best_epoch = epoch
                        if isinstance(self.model, nn.DataParallel):
                            stage1_best_model_state = self.model.module.state_dict()
                        else:
                            stage1_best_model_state = self.model.state_dict()

                        logger.info(f"ğŸ’ New Best Stage1 - Val Acc: {val_acc:.4f}")

                self._print_main_summary(epoch, is_stage1)

                # å­¦ä¹ ç‡è°ƒåº¦
                if self.config['train']['scheduler'] == 'auto':
                    val_loss = self.epoch_results[epoch]['val'].get('gnn', {}).get('loss_all', 0)
                    self.lr_scheduler.step(val_loss)
                    self.lr_scheduler_mask.step(val_loss)
                else:
                    self.lr_scheduler.step()
                    self.lr_scheduler_mask.step()

            # é˜¶æ®µ1ç»“æŸï¼šåŠ è½½æœ€ä½³æ¨¡å‹ä½œä¸ºé˜¶æ®µ2èµ·ç‚¹
            if epoch == stage_transition - 1 and self.rank == 0 and stage1_best_model_state:
                logger.info(f"\n{'='*80}")
                logger.info(f"âœ… Stage 1 Completed! Loading best model as Stage 2 starting point")
                logger.info(f"   Best Stage1 Epoch: {stage1_best_epoch + 1}")
                logger.info(f"   Best Stage1 Val Acc: {stage1_best_val_acc:.4f}")
                logger.info(f"{'='*80}\n")

                # é‡ç½®é˜¶æ®µ2è¿½è¸ªå™¨
                self.best_val_acc = 0.0
                self.best_test_acc = 0.0
                self.best_epoch = -1
    
    def _train_main_epoch(self, epoch: int, train_loader: DataLoader, is_stage1: bool):
        """ä¸»è®­ç»ƒçš„å•ä¸ªepochï¼ˆMaskå’ŒGNNäº¤æ›¿è®­ç»ƒï¼‰"""
        self.model.train()
        self.mask.train()

        # æŸå¤±å’Œå‡†ç¡®ç‡ç´¯ç§¯å™¨
        losses_mask = {
            'all': [], 'Intrinsic': [], 'Spurious': [], 'spurious_fusion': [],
            'intrinsic_fusion': [], 'sparsity_reg': []
        }
        losses_gnn = {
            'all': [], 'Intrinsic': [], 'spurious_fusion': [], 'l1_reg': []
        }
        accs_mask = {}
        accs_gnn = {}

        for batch_idx, (data, _, label) in enumerate(train_loader):
            self.global_step += 1
            label = label.to(self.device)

            # æ ¹æ®é˜¶æ®µæ„å»ºæ•°æ®å’Œç‰¹å¾
            if is_stage1:
                # é˜¶æ®µ1ï¼šä»…ä½¿ç”¨åŸå§‹å›¾
                data = data.to(self.device)
                x_features = self._extract_features(data)
            else:
                # é˜¶æ®µ2ï¼šæ„å»ºå¤§å›¾ï¼ˆAnchor + Negativesï¼‰
                large_data, large_edge = self.large_graph_builder.build_large_graph(
                    batch_data=data,
                    batch_labels=label,
                    base_edge=self.edge_prior_mask.cpu(),
                    all_data=self.all_data,
                    all_labels=self.all_labels
                )
                large_data = large_data.to(self.device)
                x_features = self._extract_features(large_data)

            # ========== æ­¥éª¤1: è®­ç»ƒMaskï¼ˆå›ºå®šGNNï¼‰ ==========
            for param in self.model.parameters():
                param.requires_grad = False

            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            masks, sparsity = mask_module(train=True)

            if is_stage1:
                result_mask = self._compute_stage1_mask_loss(
                    x_features, masks, label, epoch, self.edge_prior_mask, is_large_graph=False
                )
            else:
                result_mask = self._compute_stage2_mask_loss(
                    x_features, masks, label, epoch, self.edge_prior_mask, is_large_graph=True
                )

            self.optimizer_mask.zero_grad()
            if 'all' in result_mask['loss'] and isinstance(result_mask['loss']['all'], torch.Tensor):
                result_mask['loss']['all'].backward()
            self.optimizer_mask.step()

            # è®°å½•MaskæŸå¤±
            for k in losses_mask.keys():
                val = result_mask['loss'].get(k, 0)
                if isinstance(val, torch.Tensor):
                    losses_mask[k].append(val.item())
                else:
                    losses_mask[k].append(float(val))
            
            for k, v in result_mask['preds'].items():
                if k not in accs_mask:
                    accs_mask[k] = []
                accs_mask[k].append(self._compute_accuracy(v, label))

            # ========== æ­¥éª¤2: è®­ç»ƒGNNï¼ˆå›ºå®šMaskï¼‰ ==========
            for param in self.model.parameters():
                param.requires_grad = True

            masks, sparsity = mask_module(train=False)
            masks = [m.detach() for m in masks]

            if is_stage1:
                result_gnn = self._compute_stage1_gnn_loss(
                    x_features, masks, label, self.edge_prior_mask, is_large_graph=False
                )
            else:
                result_gnn = self._compute_stage2_gnn_loss(
                    x_features, masks, label, self.edge_prior_mask, is_large_graph=True
                )

            self.optimizer.zero_grad()
            if 'all' in result_gnn['loss'] and isinstance(result_gnn['loss']['all'], torch.Tensor):
                result_gnn['loss']['all'].backward()
            self.optimizer.step()

            # è®°å½•GNNæŸå¤±
            for k in losses_gnn.keys():
                val = result_gnn['loss'].get(k, 0)
                if isinstance(val, torch.Tensor):
                    losses_gnn[k].append(val.item())
                else:
                    losses_gnn[k].append(float(val))
            
            for k, v in result_gnn['preds'].items():
                if k not in accs_gnn:
                    accs_gnn[k] = []
                accs_gnn[k].append(self._compute_accuracy(v, label))

            # æ›´æ–°å½“å‰æ©ç ç»Ÿè®¡
            if self.rank == 0:
                self.current_mask_sums = {
                    'node': masks[0].sum().item(),
                    'edge': masks[1].sum().item()
                }

        # Epochç»“æŸï¼Œä¿å­˜ç»“æœ
        if self.rank == 0:
            train_res = {'mask': {}, 'gnn': {}}

            for k in losses_mask.keys():
                train_res['mask'][k] = float(np.mean(losses_mask[k])) if len(losses_mask[k]) > 0 else 0.0
            for k in losses_gnn.keys():
                train_res['gnn'][k] = float(np.mean(losses_gnn[k])) if len(losses_gnn[k]) > 0 else 0.0
            for k, v in accs_mask.items():
                train_res['mask'][f'acc_{k}'] = float(np.mean(v)) if len(v) > 0 else 0.0
            for k, v in accs_gnn.items():
                train_res['gnn'][f'acc_{k}'] = float(np.mean(v)) if len(v) > 0 else 0.0

            self.epoch_results[epoch]['train'] = train_res
    
    def _eval_main_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        """ä¸»è®­ç»ƒé˜¶æ®µçš„è¯„ä¼°"""
        self.model.eval()
        self.mask.eval()

        all_outputs = []
        all_labels = []

        for data, _, label in data_loader:
            data = data.to(self.device)
            label = label.to(self.device)

            x_features = self._extract_features(data)
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            masks, probs, sparsity = mask_module(train=False, return_probs=True)

            model_module = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
            outputs = model_module.prediction_intrinsic_path(
                x_features, self.edge_prior_mask, masks, is_large_graph=False
            )

            all_outputs.append(outputs)
            all_labels.append(label)

        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        metrics = compute_binary_metrics(all_outputs, all_labels)
        self.epoch_results[epoch][phase] = metrics

        # ä»…åœ¨é˜¶æ®µ2æ›´æ–°æœ€ç»ˆè¯„ä¼°çš„æœ€ä½³æ¨¡å‹
        stage_transition = self.config['train']['stage_transition_epoch']
        if phase == 'val' and epoch >= stage_transition and metrics['accuracy'] > self.best_val_acc:
            self.best_val_acc = metrics['accuracy']
            self.best_epoch = epoch

            if 'test' in self.epoch_results[epoch]:
                self.best_test_acc = self.epoch_results[epoch]['test']['accuracy']
                logger.info(
                    f"ğŸ’ New Best Stage2 - Val Acc: {metrics['accuracy']:.4f}, "
                    f"Val AUC: {metrics['auc']:.4f}, Test Acc: {self.best_test_acc:.4f}"
                )
            else:
                logger.info(
                    f"ğŸ’ New Best Stage2 Val - Acc: {metrics['accuracy']:.4f}, "
                    f"AUC: {metrics['auc']:.4f}, F1: {metrics['f1']:.4f}"
                )
    
    # ============================================================================
    # æŸå¤±è®¡ç®—å‡½æ•°
    # ============================================================================
    
    def _compute_stage1_mask_loss(self, x, masks, label, epoch, edge_prior_mask, is_large_graph):
        """
        é˜¶æ®µ1 MaskæŸå¤±ï¼šå†…åœ¨å­å›¾ + è™šå‡å­å›¾
        
        ç›®æ ‡ï¼š
        - å†…åœ¨å­å›¾ï¼šå‡†ç¡®é¢„æµ‹æ ‡ç­¾
        - è™šå‡å­å›¾ï¼šæœ€å¤§åŒ–ç†µï¼ˆæ— åˆ¤åˆ«åŠ›ï¼‰
        - ç¨€ç–æ€§ï¼šç›´æ¥æƒ©ç½šå› æœç‰¹å¾æ•°é‡
        """
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # å†…åœ¨å­å›¾é¢„æµ‹
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        # è™šå‡å­å›¾ï¼ˆç†µæŸå¤±ï¼‰
        y_spu = model.prediction_spurious_path(x, edge_prior_mask, masks, is_large_graph)
        loss_spu = self._entropy_loss(y_spu)
        
        # ç¨€ç–æ€§æ­£åˆ™ï¼ˆç®€æ´ç‰ˆï¼‰
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(
            lambda_reg=self.config['train']['loss_weights']['lambda_sparsity'],
            epoch=epoch - self.config['train']['pre_epoch'],
            max_epochs=self.config['train']['num_epoch'] - self.config['train']['pre_epoch'],
            warmup_epochs=self.config['train']['sparsity_warmup_epochs']
        )
        
        # ç»„åˆæŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = loss_weights['L_pred'] * loss_pred + loss_weights['L_spu'] * loss_spu + reg_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'Spurious': loss_spu,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'Intrinsic': y_pred,
                'Spurious': y_spu
            }
        }
    
    def _compute_stage2_mask_loss(self, x, masks, label, epoch, edge_prior_mask, is_large_graph):
        """
        é˜¶æ®µ2 MaskæŸå¤±ï¼šèåˆå›¾å¹²æ‰°æµ‹è¯•
        
        ç›®æ ‡ï¼š
        - å†…åœ¨å­å›¾ï¼šå‡†ç¡®é¢„æµ‹æ ‡ç­¾
        - è™šå‡èåˆï¼šæµ‹è¯•ä¸å˜æ€§ï¼ˆæ›¿æ¢è™šå‡èŠ‚ç‚¹åä»èƒ½é¢„æµ‹æ­£ç¡®ï¼‰
        - å†…åœ¨èåˆï¼šæµ‹è¯•æ•æ„Ÿæ€§ï¼ˆæ›¿æ¢å› æœèŠ‚ç‚¹åé¢„æµ‹ç¿»è½¬ï¼‰
        """
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # å†…åœ¨å­å›¾
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        # è™šå‡èåˆï¼ˆæµ‹è¯•ä¸å˜æ€§ï¼‰
        y_inv = model.prediction_spurious_fusion(x, edge_prior_mask, masks, is_large_graph)
        loss_inv = self.criterion(y_inv, label).mean()
        
        # å†…åœ¨èåˆï¼ˆæµ‹è¯•æ•æ„Ÿæ€§ï¼‰
        y_sen = model.prediction_intrinsic_fusion(x, edge_prior_mask, masks, is_large_graph)
        loss_sen = self.criterion(y_sen, 1 - label).mean()
        
        # ç¨€ç–æ€§æ­£åˆ™
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(
            lambda_reg=self.config['train']['loss_weights']['lambda_sparsity'],
            epoch=epoch - self.config['train']['pre_epoch'],
            max_epochs=self.config['train']['num_epoch'] - self.config['train']['pre_epoch'],
            warmup_epochs=self.config['train']['sparsity_warmup_epochs']
        )
        
        # ç»„åˆæŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = (
            loss_weights['L_inv'] * loss_inv + 
            loss_weights['L_sen'] * loss_sen + 
            loss_weights['L_pred'] * loss_pred + 
            reg_loss
        )
        
        return {
            'loss': {
                'all': loss_all,
                'spurious_fusion': loss_inv,
                'intrinsic_fusion': loss_sen,
                'Intrinsic': loss_pred,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'spurious_fusion': y_inv,
                'intrinsic_fusion': y_sen,
                'Intrinsic': y_pred
            }
        }
    
    def _compute_stage1_gnn_loss(self, x, masks, label, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ1 GNNæŸå¤±ï¼šå†…åœ¨å­å›¾é¢„æµ‹"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_pred + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'l1_reg': l1_loss
            },
            'preds': {
                'Intrinsic': y_pred
            }
        }
    
    def _compute_stage2_gnn_loss(self, x, masks, label, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ2 GNNæŸå¤±ï¼šå†…åœ¨å­å›¾ + è™šå‡èåˆ"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        y_inv = model.prediction_spurious_fusion(x, edge_prior_mask, masks, is_large_graph) 
        loss_inv = self.criterion(y_inv, label).mean()
        
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_pred + loss_inv + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'spurious_fusion': loss_inv,
                'l1_reg': l1_loss
            },
            'preds': {
                'Intrinsic': y_pred,
                'spurious_fusion': y_inv
            }
        }
    
    # ============================================================================
    # è¾…åŠ©å‡½æ•°
    # ============================================================================
    
    def _extract_features(self, data: torch.Tensor, batch_size: int = 32) -> torch.Tensor:
        """
        ä½¿ç”¨å†»ç»“çš„DenseNetæ‰¹é‡æå–ç‰¹å¾
        
        Args:
            data: è¾“å…¥æ•°æ® [B, P, 1, D, H, W]
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé¿å…OOMï¼‰
            
        Returns:
            ç‰¹å¾å¼ é‡ [B, P, feature_dim]
        """
        B = data.shape[0]
        total_P = data.shape[1]

        data_reshaped = data.view(-1, 1, data.shape[3], data.shape[4], data.shape[5])
        total_patches = data_reshaped.shape[0]

        # æ‰¹é‡æå–ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
        all_features = []
        with torch.no_grad():
            for i in range(0, total_patches, batch_size):
                batch = data_reshaped[i:i+batch_size]
                features_batch = self.densenet_model(batch)
                all_features.append(features_batch.cpu())
                del features_batch
                torch.cuda.empty_cache()

        # åœ¨CPUä¸Šæ‹¼æ¥ï¼Œå†ç§»å›GPU
        features = torch.cat(all_features, dim=0).to(self.device)
        features = features.view(B, total_P, -1)
        return features
    
    def _compute_l1_regularization(self) -> torch.Tensor:
        """è®¡ç®—GNNå‚æ•°çš„L1æ­£åˆ™åŒ–"""
        l1_reg = torch.tensor(0., device=self.device)
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        for name, param in model.named_parameters():
            if 'mlp_causal.0.weight' in name:
                l1_reg += torch.sum(torch.abs(param))
        
        return self.lambda_l1 * l1_reg
    
    def _entropy_loss(self, logits: torch.Tensor) -> torch.Tensor:
        """
        ç†µæŸå¤±ï¼ˆç”¨äºè™šå‡å­å›¾ï¼‰
        
        ç›®æ ‡ï¼šæœ€å¤§åŒ–é¢„æµ‹ç†µï¼Œä½¿è™šå‡å­å›¾æ— åˆ¤åˆ«åŠ›
        """
        probs = torch.softmax(logits, dim=1)
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
        return -entropy.mean()  # è´Ÿå·ï¼šæœ€å¤§åŒ–ç†µ
    
    def _compute_accuracy(self, outputs: torch.Tensor, labels: torch.Tensor) -> float:
        """è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡"""
        _, predicted = torch.max(outputs, 1)
        acc = (predicted == labels).float().mean().item()
        return acc
    
    # ============================================================================
    # æ—¥å¿—æ‰“å°
    # ============================================================================
    
    def _print_pretrain_summary(self, epoch: int):
        """æ‰“å°é¢„è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})

        logger.info("="*80)
        logger.info(f"ğŸ“š Epoch {epoch+1}/{self.config['train']['pre_epoch']} [Pre-training]")
        logger.info("-"*80)
        logger.info(f"Train - Loss: {train_res.get('loss_all', 0):.4f}, "
                   f"Acc: {train_res.get('acc_Intrinsic', 0):.4f}")

        logger.info(f"Val   - Acc: {val_res.get('accuracy', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}, "
                   f"F1: {val_res.get('f1', 0):.4f}")
        logger.info(f"        Sens: {val_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {val_res.get('specificity', 0):.4f}")

        logger.info(f"Test  - Acc: {test_res.get('accuracy', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}, "
                   f"F1: {test_res.get('f1', 0):.4f}")
        logger.info(f"        Sens: {test_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {test_res.get('specificity', 0):.4f}")

        logger.info("="*80 + "\n")
    
    def _print_main_summary(self, epoch: int, is_stage1: bool):
        """æ‰“å°ä¸»è®­ç»ƒé˜¶æ®µçš„è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})

        mask_res = train_res.get('mask', {})
        gnn_res = train_res.get('gnn', {})

        stage_name = "Stage 1 (Causal Separation)" if is_stage1 else "Stage 2 (Robustness Enhancement)"

        logger.info("="*80)
        logger.info(f"ğŸ¯ Epoch {epoch+1}/{self.config['train']['num_epoch']} [{stage_name}]")
        logger.info("-"*80)

        # å®˜æ–¹è¯„ä¼°æŒ‡æ ‡
        logger.info(f"ğŸ“Š Validation Metrics:")
        logger.info(f"   Acc: {val_res.get('accuracy', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}, "
                   f"F1: {val_res.get('f1', 0):.4f}")
        logger.info(f"   Sens: {val_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {val_res.get('specificity', 0):.4f}, "
                   f"Prec: {val_res.get('precision', 0):.4f}")

        logger.info(f"\nğŸ“Š Test Metrics:")
        logger.info(f"   Acc: {test_res.get('accuracy', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}, "
                   f"F1: {test_res.get('f1', 0):.4f}")
        logger.info(f"   Sens: {test_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {test_res.get('specificity', 0):.4f}, "
                   f"Prec: {test_res.get('precision', 0):.4f}")

        # Maskè®­ç»ƒè¯¦æƒ…
        logger.info(f"\nğŸ­ Mask Training:")
        logger.info(f"   Total Loss: {mask_res.get('all', 0):.4f}")
        if is_stage1:
            logger.info(f"     â”œâ”€ Intrinsic:  {mask_res.get('Intrinsic', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_Intrinsic', 0):.2%})")
            logger.info(f"     â”œâ”€ Spurious:   {mask_res.get('Spurious', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_Spurious', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:   {mask_res.get('sparsity_reg', 0):.4f}")
        else:
            logger.info(f"     â”œâ”€ Intrinsic:        {mask_res.get('Intrinsic', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_Intrinsic', 0):.2%})")
            logger.info(f"     â”œâ”€ Spurious Fusion:  {mask_res.get('spurious_fusion', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_spurious_fusion', 0):.2%})")
            logger.info(f"     â”œâ”€ Intrinsic Fusion: {mask_res.get('intrinsic_fusion', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_intrinsic_fusion', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:         {mask_res.get('sparsity_reg', 0):.4f}")

        # GNNè®­ç»ƒè¯¦æƒ…
        logger.info(f"\nğŸ§  GNN Training:")
        logger.info(f"   Total Loss: {gnn_res.get('all', 0):.4f}")
        logger.info(f"     â”œâ”€ Intrinsic: {gnn_res.get('Intrinsic', 0):.4f} "
                   f"(Acc: {gnn_res.get('acc_Intrinsic', 0):.2%})")
        if not is_stage1:
            logger.info(f"     â”œâ”€ Spurious Fusion: {gnn_res.get('spurious_fusion', 0):.4f} "
                       f"(Acc: {gnn_res.get('acc_spurious_fusion', 0):.2%})")
        logger.info(f"     â””â”€ L1 Reg:     {gnn_res.get('l1_reg', 0):.4f}")

        # å­¦ä¹ ç‡
        lr_gnn = self.optimizer.param_groups[0]['lr']
        lr_mask = self.optimizer_mask.param_groups[0]['lr']
        logger.info(f"\nâš™ï¸  Learning Rates:")
        logger.info(f"   GNN:  {lr_gnn:.6f}")
        logger.info(f"   Mask: {lr_mask:.6f}")

        # æ©ç ç¨€ç–åº¦ç»Ÿè®¡
        if self.current_mask_sums:
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            total_nodes = mask_module.P
            total_edges = int(mask_module.learnable_mask.sum().item())

            node_sum = int(self.current_mask_sums.get('node', 0))
            edge_sum = int(self.current_mask_sums.get('edge', 0))

            node_pct = node_sum / total_nodes * 100 if total_nodes > 0 else 0
            edge_pct = edge_sum / total_edges * 100 if total_edges > 0 else 0

            logger.info(f"\nğŸ­ Mask Sparsity:")
            logger.info(f"   Nodes: {node_sum}/{total_nodes} ({node_pct:.1f}%)")
            logger.info(f"   Edges: {edge_sum}/{total_edges} ({edge_pct:.1f}%)")

        logger.info("="*80 + "\n")
    
    def _final_evaluation(self, test_loader: DataLoader) -> Dict[str, float]:
        """æœ€ç»ˆè¯„ä¼°å¹¶è¿”å›ç»“æœ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ Final Evaluation")
        logger.info("="*80)

        # ç¡®ä¿test_accè¢«æ­£ç¡®è®°å½•
        if self.best_test_acc == 0.0 and self.best_epoch >= 0:
            if 'test' in self.epoch_results.get(self.best_epoch, {}):
                self.best_test_acc = self.epoch_results[self.best_epoch]['test']['accuracy']
                logger.info(f"â„¹ï¸  Retrieved test acc from epoch {self.best_epoch + 1}")

        results = {
            'fold': self.fold,
            'best_epoch': self.best_epoch,
            'val_acc': self.best_val_acc,
            'test_acc': self.best_test_acc
        }

        logger.info(f"Best Epoch:    {self.best_epoch + 1}")
        logger.info(f"Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"Best Test Acc: {self.best_test_acc:.4f}")
        logger.info("="*80)

        return results