"""
ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
ä» main_causal.py ç²¾ç‚¼æå–ï¼Œç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¼˜é›…
"""

import os
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR
from typing import Dict, Any, Optional, Tuple
import numpy as np
from sklearn.metrics import roc_auc_score

from utils.checkpoint import CheckpointManager
from models.causal_net import CausalNet
from models.causal_mask import CausalMask
from data.large_graph_builder import LargeGraphBuilder

logger = logging.getLogger(__name__)


class CausalTrainer:
    """
    ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
    
    ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š
    1. é¢„è®­ç»ƒï¼šè®­ç»ƒæ•´ä½“é¢„æµ‹ (40 epochs)
    2. é˜¶æ®µ1ï¼šMask+GNNè”åˆè®­ç»ƒï¼ˆä¸å˜æ€§+å˜å¼‚æ€§ï¼‰ (40 epochs)
    3. é˜¶æ®µ2ï¼šMask+GNNè”åˆè®­ç»ƒï¼ˆå› æœæ€§+åäº‹å®ï¼‰ (60 epochs)
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        fold: int,
        densenet_model: nn.Module,
        edge_matrix: np.ndarray,
        checkpoint_manager: CheckpointManager,
        work_dir: str,
        device: str = 'cuda',
        rank: int = 0
    ):
        self.config = config
        self.fold = fold
        self.densenet_model = densenet_model.to(device)
        self.edge_matrix = torch.FloatTensor(edge_matrix).to(device)
        self.checkpoint_manager = checkpoint_manager
        self.work_dir = work_dir
        self.device = device
        self.rank = rank
        
        # å†»ç»“DenseNet
        for param in self.densenet_model.parameters():
            param.requires_grad = False
        self.densenet_model.eval()
        
        # æ¨¡å‹
        self.model = None
        self.mask = None
        
        # ä¼˜åŒ–å™¨
        self.optimizer = None
        self.optimizer_mask = None
        self.lr_scheduler = None
        self.lr_scheduler_mask = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0
        self.best_val_acc = 0.0
        self.best_test_acc = 0.0
        self.best_epoch = -1
        self.best_model_state = None
        self.epoch_results = {}
        self.current_mask_sums = {}
        self.large_graph_builder = LargeGraphBuilder(
        num_neg_samples=4,
        sampling_strategy='opposite_label',
        random_seed=config['seed']
    )
        self.all_data = None
        self.all_labels = None        
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(reduction='none')
        self.lambda_l1 = config['train']['loss_weights']['lambda_l1']
        
        logger.info(f"âœ… Trainer initialized for Fold {fold}")
    
    def _build_models(self):
        """æ„å»ºæ¨¡å‹"""
        # ä¸»GNNæ¨¡å‹
        self.model = CausalNet(
            num_class=2,
            feature_dim=self.densenet_model.feature_dim,
            hidden1=self.config['model']['args']['hidden1'],
            hidden2=self.config['model']['args']['hidden2'],
            kernels=self.config['model']['args'].get('kernels', [2])
        ).to(self.device)
        
        # å› æœæ©ç æ¨¡å‹
        self.mask = CausalMask(
            num_patches=self.edge_matrix.shape[0],
            edge_matrix=self.edge_matrix,
            gumble_tau=self.config['misc']['gumble_tau']
        ).to(self.device)
        
        # DataParallel
        if torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            self.mask = nn.DataParallel(self.mask)
        
        logger.info("âœ… Models built")
    
    def _setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        if self.config['train']['optimizer'] == 'SGD':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=self.config['train']['base_lr'],
                momentum=0.9,
                nesterov=self.config['train'].get('nesterov', True),
                weight_decay=self.config['train']['weight_decay']
            )
            self.optimizer_mask = optim.SGD(
                self.mask.parameters(),
                lr= self.config['train']['base_lr_mask'],
                momentum=0.9,
                nesterov=self.config['train'].get('nesterov', True),
                weight_decay=self.config['train']['weight_decay']
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config['train']['scheduler'] == 'auto':
            self.lr_scheduler = ReduceLROnPlateau(
                self.optimizer,
                verbose=(self.rank == 0),
                patience=self.config['train']['stepsize'],
                factor=self.config['train']['gamma']
            )
            self.lr_scheduler_mask = ReduceLROnPlateau(
                self.optimizer_mask,
                verbose=(self.rank == 0),
                patience=self.config['train']['stepsize'],
                factor=self.config['train']['gamma']
            )
        elif self.config['train']['scheduler'] == 'step':
            self.lr_scheduler = StepLR(
                self.optimizer,
                step_size=self.config['train']['stepsize'],
                gamma=self.config['train']['gamma']
            )
            self.lr_scheduler_mask = StepLR(
                self.optimizer_mask,
                step_size=self.config['train']['stepsize'],
                gamma=self.config['train']['gamma']
            )
        
        logger.info("âœ… Optimizers configured")
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader
    ) -> Dict[str, float]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ Training Start - Fold {self.fold}")
        logger.info(f"{'='*80}\n")
        
        # é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º
        if self.all_data is None:
            logger.info("é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º...")
            dataset = train_loader.dataset
            if hasattr(dataset, 'dataset'):
                dataset = dataset.dataset
            self.all_data = np.array(dataset.all_patches)
            self.all_labels = np.array(dataset.labels)
            logger.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆ: {self.all_data.shape}")        
        
        # åˆå§‹åŒ–
        self._build_models()
        self._setup_optimizers()
        
        # é˜¶æ®µ1: é¢„è®­ç»ƒ (40 epochs)
        pre_epochs = self.config['train']['pre_epoch']
        if pre_epochs > 0:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“š Phase 1: Pre-training (Whole Prediction)")
            logger.info("="*80)
            self._pretrain_phase(train_loader, val_loader, test_loader, pre_epochs)
        
        # é˜¶æ®µ2+3: ä¸»è®­ç»ƒ (100 epochs)
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ Phase 2+3: Main Training (Mask + GNN)")
        logger.info("="*80)
        self._main_training(train_loader, val_loader, test_loader)
        
        # æœ€ç»ˆè¯„ä¼°
        final_results = self._final_evaluation(test_loader)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… Training Completed - Fold {self.fold}")
        logger.info(f"   Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"   Best Test Acc: {self.best_test_acc:.4f}")
        logger.info(f"{'='*80}\n")
        
        return final_results
    
    #==================== é¢„è®­ç»ƒé˜¶æ®µ ====================
    
    def _pretrain_phase(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        num_epochs: int
    ):
        """é¢„è®­ç»ƒé˜¶æ®µ"""
        for epoch in range(num_epochs):
            if self.rank == 0:
                self.epoch_results[epoch] = {}
            
            # è®­ç»ƒ
            self._train_pretrain_epoch(epoch, train_loader)
            
            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_pretrain_epoch(epoch, val_loader, 'val')
                    self._eval_pretrain_epoch(epoch, test_loader, 'test')
                
                self._print_pretrain_summary(epoch)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.rank == 0 and self.best_model_state:
            logger.info(f"âœ… Loading best pretrain model (Epoch {self.best_epoch+1})")
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(self.best_model_state)
            else:
                self.model.load_state_dict(self.best_model_state)
    
    def _train_pretrain_epoch(self, epoch: int, train_loader: DataLoader):
        """é¢„è®­ç»ƒçš„ä¸€ä¸ªepoch"""
        self.model.train()
        
        losses = []
        accuracies = []
        
        for data, _, label in train_loader:
            self.global_step += 1
            
            # è½¬æ¢æ•°æ®
            data = data.to(self.device)
            label = label.to(self.device)
            
            # æå–ç‰¹å¾
            x_features = self._extract_features(data)
            
            # æ•´ä½“é¢„æµ‹
            outputs = self.model.module.prediction_whole(x_features, self.edge_matrix) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_matrix)
            
            loss = self.criterion(outputs, label).mean()
            l1_loss = self._compute_l1_regularization()
            loss_total = loss + l1_loss
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss_total.backward()
            self.optimizer.step()
            
            # è®°å½•
            if self.rank == 0:
                losses.append(loss.item())
                _, predicted = torch.max(outputs, 1)
                acc = (predicted == label).float().mean().item()
                accuracies.append(acc)
        
        # ä¿å­˜ç»“æœ
        if self.rank == 0:
            self.epoch_results[epoch]['train'] = {
                'loss_all': np.mean(losses),
                'acc_invariance': np.mean(accuracies)
            }
    
    def _eval_pretrain_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        """é¢„è®­ç»ƒè¯„ä¼°"""
        self.model.eval()
        
        all_outputs = []
        all_labels = []
        losses = []
        
        for data, _, label in data_loader:
            data = data.to(self.device)
            label = label.to(self.device)
            
            x_features = self._extract_features(data)
            outputs = self.model.module.prediction_whole(x_features, self.edge_matrix) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_matrix)
            
            loss = self.criterion(outputs, label).mean()
            
            all_outputs.append(outputs)
            all_labels.append(label)
            losses.append(loss.item())
        
        # è®¡ç®—æŒ‡æ ‡
        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        _, predicted = torch.max(all_outputs, 1)
        acc = (predicted == all_labels).float().mean().item()
        
        # AUC
        probs = torch.softmax(all_outputs, dim=1)[:, 1].cpu().numpy()
        try:
            auc = roc_auc_score(all_labels.cpu().numpy(), probs)
        except:
            auc = 0.0
        
        # ä¿å­˜ç»“æœ
        self.epoch_results[epoch][phase] = {
            'loss_all': np.mean(losses),
            'acc_official': acc,
            'auc': auc
        }
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if phase == 'val' and acc > self.best_val_acc:
            self.best_val_acc = acc
            self.best_epoch = epoch
            if isinstance(self.model, nn.DataParallel):
                self.best_model_state = self.model.module.state_dict()
            else:
                self.best_model_state = self.model.state_dict()
            
            logger.info(f"ğŸ’ New Best Val Acc: {acc:.4f}")
    
    #==================== ä¸»è®­ç»ƒé˜¶æ®µ ====================
    
    def _main_training(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader
    ):
        """ä¸»è®­ç»ƒé˜¶æ®µï¼ˆé˜¶æ®µ1+é˜¶æ®µ2ï¼‰"""
        start_epoch = self.config['train']['pre_epoch']
        num_epochs = self.config['train']['num_epoch']
        stage_transition = self.config['train']['stage_transition_epoch']
        
        for epoch in range(start_epoch, num_epochs):
            is_stage1 = epoch < stage_transition
            
            if self.rank == 0:
                self.epoch_results[epoch] = {}
            
            # è®­ç»ƒ
            self._train_main_epoch(epoch, train_loader, is_stage1)
            
            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_main_epoch(epoch, val_loader, 'val')
                    self._eval_main_epoch(epoch, test_loader, 'test')
                
                self._print_main_summary(epoch, is_stage1)
                
                # æ›´æ–°å­¦ä¹ ç‡
                if self.config['train']['scheduler'] == 'auto':
                    val_loss = self.epoch_results[epoch]['val'].get('gnn', {}).get('loss_all', 0)
                    self.lr_scheduler.step(val_loss)
                    self.lr_scheduler_mask.step(val_loss)
                else:
                    self.lr_scheduler.step()
                    self.lr_scheduler_mask.step()
    
    def _train_main_epoch(self, epoch: int, train_loader: DataLoader, is_stage1: bool):
        """ä¸»è®­ç»ƒçš„ä¸€ä¸ªepoch"""
        self.model.train()
        self.mask.train()
        
        # æŸå¤±è®°å½•å™¨
        losses_mask = {'all': [], 'invariance': [], 'variability': [], 'causal': [], 
                       'counterfactual': [], 'sparsity_reg': []}
        losses_gnn = {'all': [], 'invariance': [], 'causal': [], 'l1_reg': []}
        accs_mask = {}
        accs_gnn = {}
        
        # è®¡ç®—æ­£åˆ™åŒ–å¼ºåº¦
        lambda_reg = 0.05 * (1 + epoch / self.config['train']['num_epoch'])
        
        for data, _, label in train_loader:
            self.global_step += 1
            
            label = label.to(self.device)
            
            # ğŸ†• æ„å»ºå¤§å›¾
            large_data, large_edge = self.large_graph_builder.build_large_graph(
                batch_data=data,
                batch_labels=label,
                base_edge=self.edge_matrix.cpu(),
                all_data=self.all_data,
                all_labels=self.all_labels
            )
            large_data = large_data.to(self.device)
            large_edge = large_edge.to(self.device)
            
            # æå–å¤§å›¾ç‰¹å¾
            x_features = self._extract_features(large_data)
            
            #========== 1. Maskè®­ç»ƒ ==========
            for param in self.model.parameters():
                param.requires_grad = False
            
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            masks, sparsity = mask_module(train=True)
            
            if is_stage1:
                result_mask = self._compute_stage1_mask_loss(x_features, masks, label, lambda_reg, large_edge)
                if self.rank == 0:
                    accs_mask.setdefault('invariance', []).append(
                        self._compute_accuracy(result_mask['preds']['invariance'], label))
                    accs_mask.setdefault('variability', []).append(
                        self._compute_accuracy(result_mask['preds']['variability'], label))
            else:
                result_mask = self._compute_stage2_mask_loss(x_features, masks, label, lambda_reg, large_edge)
                if self.rank == 0:
                    accs_mask.setdefault('invariance', []).append(
                        self._compute_accuracy(result_mask['preds']['invariance'], label))
                    accs_mask.setdefault('causal', []).append(
                        self._compute_accuracy(result_mask['preds']['causal'], label))
                    accs_mask.setdefault('counterfactual', []).append(
                        self._compute_accuracy(result_mask['preds']['counterfactual'], label))
            
            self.optimizer_mask.zero_grad()
            result_mask['loss']['all'].backward()
            self.optimizer_mask.step()
            
            if self.rank == 0:
                for k, v in result_mask['loss'].items():
                    if k != 'all':
                        losses_mask[k].append(v.item())
                losses_mask['all'].append(result_mask['loss']['all'].item())
            
            #========== 2. GNNè®­ç»ƒ ==========
            for param in self.model.parameters():
                param.requires_grad = True
            
            masks, sparsity = mask_module(train=False)
            masks = [m.detach() for m in masks]
            
            if is_stage1:
                result_gnn = self._compute_stage1_gnn_loss(x_features, masks, label, large_edge)
                if self.rank == 0:
                    accs_gnn.setdefault('invariance', []).append(
                        self._compute_accuracy(result_gnn['preds']['invariance'], label))
            else:
                result_gnn = self._compute_stage2_gnn_loss(x_features, masks, label, large_edge)
                if self.rank == 0:
                    accs_gnn.setdefault('invariance', []).append(
                        self._compute_accuracy(result_gnn['preds']['invariance'], label))
                    accs_gnn.setdefault('causal', []).append(
                        self._compute_accuracy(result_gnn['preds']['causal'], label))
            
            self.optimizer.zero_grad()
            result_gnn['loss']['all'].backward()
            self.optimizer.step()
            
            if self.rank == 0:
                for k, v in result_gnn['loss'].items():
                    if k != 'all':
                        losses_gnn[k].append(v.item())
                losses_gnn['all'].append(result_gnn['loss']['all'].item())
                
                # è®°å½•æ©ç ç»Ÿè®¡
                self.current_mask_sums = {
                    'node': masks[0].sum().item(),
                    'edge': masks[1].sum().item()
                }
        
        # ä¿å­˜epochç»“æœ
        if self.rank == 0:
            train_res = {
                'mask': {k: np.mean(v) if v else 0 for k, v in losses_mask.items()},
                'gnn': {k: np.mean(v) if v else 0 for k, v in losses_gnn.items()}
            }
            for k, v in accs_mask.items():
                train_res['mask'][f'acc_{k}'] = np.mean(v)
            for k, v in accs_gnn.items():
                train_res['gnn'][f'acc_{k}'] = np.mean(v)
            
            self.epoch_results[epoch]['train'] = train_res
    
    def _eval_main_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        """ä¸»è®­ç»ƒè¯„ä¼°"""
        self.model.eval()
        self.mask.eval()
        
        all_outputs = []
        all_labels = []
        
        for data, _, label in data_loader:
            batch_data = data
            data = data.to(self.device)
            label = label.to(self.device)
            
            x_features = self._extract_features(data)
            
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            masks, probs, sparsity = mask_module(train=False, return_probs=True)
            
            model_module = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
            # è¯„ä¼°æ—¶ä½¿ç”¨å°å›¾
            B, P = batch_data.shape[0], batch_data.shape[1]
            small_edge = self.edge_matrix.unsqueeze(0).repeat(B, 1, 1)
            outputs = model_module.prediction_causal_invariance(x_features, small_edge, masks, is_large_graph=False)
            
            all_outputs.append(outputs)
            all_labels.append(label)
        
        # è®¡ç®—æŒ‡æ ‡
        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        _, predicted = torch.max(all_outputs, 1)
        acc = (predicted == all_labels).float().mean().item()
        
        probs = torch.softmax(all_outputs, dim=1)[:, 1].cpu().numpy()
        try:
            auc = roc_auc_score(all_labels.cpu().numpy(), probs)
        except:
            auc = 0.0
        
        self.epoch_results[epoch][phase] = {
            'acc_official': acc,
            'auc': auc
        }
        
        # æ›´æ–°æœ€ä½³
        if phase == 'val' and acc > self.best_val_acc:
            self.best_val_acc = acc
            self.best_epoch = epoch
            logger.info(f"ğŸ’ New Best Val Acc: {acc:.4f}")
        
        if phase == 'test' and acc > self.best_test_acc:
            self.best_test_acc = acc
    
    #==================== æŸå¤±è®¡ç®— ====================
    
    def _compute_stage1_mask_loss(self, x, masks, label, lambda_reg, edge):
        """é˜¶æ®µ1 MaskæŸå¤±ï¼šä¸å˜æ€§ + å˜å¼‚æ€§"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # ä¸å˜æ€§
        yci = model.prediction_causal_invariance(x, edge, masks, True)
        loss_ci = self.criterion(yci, label).mean()
        
        # å˜å¼‚æ€§ï¼ˆç†µæŸå¤±ï¼‰
        ycv = model.prediction_causal_variability(x, edge, masks, True)
        loss_cv = self._entropy_loss(ycv)
        
        # ç¨€ç–æ€§æ­£åˆ™
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(lambda_reg=lambda_reg)
        
        # æ€»æŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = loss_weights['LCI'] * loss_ci + loss_weights['LCV'] * loss_cv + reg_loss
        
        return {
            'loss': {
                'all': loss_all,
                'invariance': loss_ci,
                'variability': loss_cv,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'invariance': yci,
                'variability': ycv
            }
        }
    
    def _compute_stage2_mask_loss(self, x, masks, label, lambda_reg, edge):
        """é˜¶æ®µ2 MaskæŸå¤±ï¼šå› æœ + åäº‹å® + ä¸å˜æ€§"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # ä¸å˜æ€§
        yci = model.prediction_causal_invariance(x, edge, masks, True)
        loss_ci = self.criterion(yci, label).mean()
        
        # å› æœæ€§
        yc = model.prediction_causal_invariance(x, edge, masks, True)  # ä½¿ç”¨ç›¸åŒæ–¹æ³•
        loss_c = self.criterion(yc, label).mean()
        
        # åäº‹å®
        yo = model.prediction_causal_variability(x, edge, masks, True)
        loss_o = self.criterion(yo, 1 - label).mean()
        
        # ç¨€ç–æ€§æ­£åˆ™
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(lambda_reg=lambda_reg)
        
        # æ€»æŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = (loss_weights['LC'] * loss_c + 
                   loss_weights['LO'] * loss_o + 
                   loss_weights['LCI'] * loss_ci + 
                   reg_loss)
        
        return {
            'loss': {
                'all': loss_all,
                'causal': loss_c,
                'counterfactual': loss_o,
                'invariance': loss_ci,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'causal': yc,
                'counterfactual': yo,
                'invariance': yci
            }
        }
    def _compute_stage1_gnn_loss(self, x, masks, label, edge):
        """é˜¶æ®µ1 GNNæŸå¤±ï¼šä¸å˜æ€§"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        yci = model.prediction_causal_invariance(x, edge, masks, True)
        loss_ci = self.criterion(yci, label).mean()
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_ci + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'invariance': loss_ci,
                'l1_reg': l1_loss
            },
            'preds': {
                'invariance': yci
            }
        }
    
    def _compute_stage2_gnn_loss(self, x, masks, label, edge):
        """é˜¶æ®µ2 GNNæŸå¤±ï¼šä¸å˜æ€§ + å› æœ"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        yci = model.prediction_causal_invariance(x, edge, masks, True)
        loss_ci = self.criterion(yci, label).mean()
        
        yc = model.prediction_causal_invariance(x, edge, masks, True)
        loss_c = self.criterion(yc, label).mean()
        
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_ci + loss_c + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'invariance': loss_ci,
                'causal': loss_c,
                'l1_reg': l1_loss
            },
            'preds': {
                'invariance': yci,
                'causal': yc
            }
        }
    
    #==================== è¾…åŠ©å‡½æ•° ====================
    
    def _extract_features(self, data: torch.Tensor, batch_size: int = 32) -> torch.Tensor:
        """ä½¿ç”¨DenseNetæ‰¹é‡æå–ç‰¹å¾ï¼ˆé¿å…OOMï¼‰"""
        B = data.shape[0]
        total_P = data.shape[1]

        data_reshaped = data.view(-1, 1, data.shape[3], data.shape[4], data.shape[5])
        total_patches = data_reshaped.shape[0]

        # æ‰¹é‡æå–ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
        all_features = []
        with torch.no_grad():
            for i in range(0, total_patches, batch_size):
                batch = data_reshaped[i:i+batch_size]
                features_batch = self.densenet_model(batch)
                all_features.append(features_batch.cpu())  # ç«‹å³ç§»åˆ°CPU
                del features_batch
                torch.cuda.empty_cache()

        # åœ¨CPUä¸Šæ‹¼æ¥ï¼Œå†ç§»å›GPU
        features = torch.cat(all_features, dim=0).to(self.device)
        features = features.view(B, total_P, -1)
        return features
    
    def _compute_l1_regularization(self) -> torch.Tensor:
        """è®¡ç®—L1æ­£åˆ™åŒ–"""
        l1_reg = torch.tensor(0., device=self.device)
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        for name, param in model.named_parameters():
            if 'mlp_causal.0.weight' in name:
                l1_reg += torch.sum(torch.abs(param))
        
        return self.lambda_l1 * l1_reg
    
    def _entropy_loss(self, logits: torch.Tensor) -> torch.Tensor:
        """ç†µæŸå¤±ï¼ˆç”¨äºå˜å¼‚æ€§ï¼‰"""
        probs = torch.softmax(logits, dim=1)
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
        return -entropy.mean()  # è´Ÿå·ä½¿å…¶æœ€å¤§åŒ–ç†µ
    
    def _compute_accuracy(self, outputs: torch.Tensor, labels: torch.Tensor) -> float:
        """è®¡ç®—å‡†ç¡®ç‡"""
        _, predicted = torch.max(outputs, 1)
        acc = (predicted == labels).float().mean().item()
        return acc
    
    #==================== æ‰“å° ====================
    
    def _print_pretrain_summary(self, epoch: int):
        """æ‰“å°é¢„è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})
        
        logger.info("="*80)
        logger.info(f"ğŸ“š Epoch {epoch+1}/{self.config['train']['pre_epoch']} [Pre-train]")
        logger.info("-"*80)
        logger.info(f"Train - Loss: {train_res.get('loss_all', 0):.4f}, "
                   f"Acc: {train_res.get('acc_invariance', 0):.4f}")
        logger.info(f"Val   - Loss: {val_res.get('loss_all', 0):.4f}, "
                   f"Acc: {val_res.get('acc_official', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}")
        logger.info(f"Test  - Acc: {test_res.get('acc_official', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}")
        logger.info("="*80 + "\n")
    
    def _print_main_summary(self, epoch: int, is_stage1: bool):
        """æ‰“å°ä¸»è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})
        
        mask_res = train_res.get('mask', {})
        gnn_res = train_res.get('gnn', {})
        
        stage_name = "Stage 1 (Invariance+Variability)" if is_stage1 else "Stage 2 (Causal+Counterfactual)"
        
        logger.info("="*80)
        logger.info(f"ğŸ¯ Epoch {epoch+1}/{self.config['train']['num_epoch']} [{stage_name}]")
        logger.info("-"*80)
        
        # å®˜æ–¹è¯„ä¼°ç»“æœ
        logger.info(f"ğŸ“Š Official Evaluation:")
        logger.info(f"   Val  - Acc: {val_res.get('acc_official', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}")
        logger.info(f"   Test - Acc: {test_res.get('acc_official', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}")
        
        # Maskè®­ç»ƒè¯¦æƒ…
        logger.info(f"\nğŸ­ Mask Training:")
        logger.info(f"   Total Loss: {mask_res.get('loss_all', 0):.4f}")
        if is_stage1:
            logger.info(f"     â”œâ”€ Invariance:  {mask_res.get('loss_invariance', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_invariance', 0):.2%})")
            logger.info(f"     â”œâ”€ Variability: {mask_res.get('loss_variability', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_variability', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:    {mask_res.get('loss_sparsity_reg', 0):.4f}")
        else:
            logger.info(f"     â”œâ”€ Invariance:     {mask_res.get('loss_invariance', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_invariance', 0):.2%})")
            logger.info(f"     â”œâ”€ Causal:         {mask_res.get('loss_causal', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_causal', 0):.2%})")
            logger.info(f"     â”œâ”€ Counterfactual: {mask_res.get('loss_counterfactual', 0):.4f} "
                       f"(Acc: {mask_res.get('acc_counterfactual', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:       {mask_res.get('loss_sparsity_reg', 0):.4f}")
        
        # GNNè®­ç»ƒè¯¦æƒ…
        logger.info(f"\nğŸ§  GNN Training:")
        logger.info(f"   Total Loss: {gnn_res.get('loss_all', 0):.4f}")
        logger.info(f"     â”œâ”€ Invariance: {gnn_res.get('loss_invariance', 0):.4f} "
                   f"(Acc: {gnn_res.get('acc_invariance', 0):.2%})")
        if not is_stage1:
            logger.info(f"     â”œâ”€ Causal:     {gnn_res.get('loss_causal', 0):.4f} "
                       f"(Acc: {gnn_res.get('acc_causal', 0):.2%})")
        logger.info(f"     â””â”€ L1 Reg:     {gnn_res.get('loss_l1_reg', 0):.4f}")
        
        # è®­ç»ƒå‚æ•°
        lr_gnn = self.optimizer.param_groups[0]['lr']
        lr_mask = self.optimizer_mask.param_groups[0]['lr']
        logger.info(f"\nâš™ï¸  Learning Rates:")
        logger.info(f"   GNN:  {lr_gnn:.6f}")
        logger.info(f"   Mask: {lr_mask:.6f}")
        
        # æ©ç ç»Ÿè®¡
        if self.current_mask_sums:
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            total_nodes = mask_module.P
            total_edges = int(mask_module.learnable_mask.sum().item())
            
            node_sum = int(self.current_mask_sums.get('node', 0))
            edge_sum = int(self.current_mask_sums.get('edge', 0))
            
            node_pct = node_sum / total_nodes * 100 if total_nodes > 0 else 0
            edge_pct = edge_sum / total_edges * 100 if total_edges > 0 else 0
            
            logger.info(f"\nğŸ­ Mask Statistics:")
            logger.info(f"   Nodes: {node_sum}/{total_nodes} ({node_pct:.1f}%)")
            logger.info(f"   Edges: {edge_sum}/{total_edges} ({edge_pct:.1f}%)")
        
        logger.info("="*80 + "\n")
    
    def _final_evaluation(self, test_loader: DataLoader) -> Dict[str, float]:
        """æœ€ç»ˆè¯„ä¼°"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ Final Evaluation")
        logger.info("="*80)
        
        # è¿™é‡Œå¯ä»¥åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
        # ä¸ºç®€åŒ–ï¼Œç›´æ¥è¿”å›æœ€ä½³ç»“æœ
        
        results = {
            'fold': self.fold,
            'best_epoch': self.best_epoch,
            'val_acc': self.best_val_acc,
            'test_acc': self.best_test_acc
        }
        
        logger.info(f"Best Epoch:    {self.best_epoch + 1}")
        logger.info(f"Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"Best Test Acc: {self.best_test_acc:.4f}")
        logger.info("="*80)
        
        return results