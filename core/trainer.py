"""
ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
ä» main_causal.py ç²¾ç‚¼æå–ï¼Œç»“æ„æ¸…æ™°ï¼Œé€»è¾‘ä¼˜é›…
"""

import os
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR
from typing import Dict, Any, Optional, Tuple
import numpy as np
from sklearn.metrics import roc_auc_score

from utils.checkpoint import CheckpointManager
from models.causal_net import CausalNet
from models.causal_mask import CausalMask
from data.large_graph_builder import LargeGraphBuilder
from utils.metrics import compute_binary_metrics, print_metrics

logger = logging.getLogger(__name__)


class CausalTrainer:
    """
    ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒå™¨
    
    ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š
    1. é¢„è®­ç»ƒï¼šè®­ç»ƒæ•´ä½“é¢„æµ‹ (40 epochs)
    2. é˜¶æ®µ1ï¼šMask+GNNè”åˆè®­ç»ƒï¼ˆå†…åœ¨å­å›¾+è™šå‡å­å›¾ï¼‰ (40 epochs)
    3. é˜¶æ®µ2ï¼šMask+GNNè”åˆè®­ç»ƒï¼ˆè™šå‡å­å›¾å¹²æ‰°æ€§+å†…åœ¨å­å›¾å¹²æ‰°ï¼‰ (60 epochs)
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        fold: int,
        densenet_model: nn.Module,
        edge_prior_mask: np.ndarray,
        checkpoint_manager: CheckpointManager,
        work_dir: str,
        device: str = 'cuda',
        rank: int = 0
    ):
        self.config = config
        self.fold = fold
        self.densenet_model = densenet_model.to(device)
        self.edge_prior_mask = torch.FloatTensor(edge_prior_mask).to(device)
        self.checkpoint_manager = checkpoint_manager
        self.work_dir = work_dir
        self.device = device
        self.rank = rank
        
        # å†»ç»“DenseNet
        for param in self.densenet_model.parameters():
            param.requires_grad = False
        self.densenet_model.eval()
        
        # æ¨¡å‹
        self.model = None
        self.mask = None
        
        # ä¼˜åŒ–å™¨
        self.optimizer = None
        self.optimizer_mask = None
        self.lr_scheduler = None
        self.lr_scheduler_mask = None
        
        # è®­ç»ƒçŠ¶æ€
        self.global_step = 0

        # é¢„è®­ç»ƒé˜¶æ®µçŠ¶æ€
        self.pretrain_best_val_acc = 0.0
        self.pretrain_best_model_state = None
        self.pretrain_best_epoch = -1

        # ä¸»è®­ç»ƒé˜¶æ®µçŠ¶æ€ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        self.best_val_acc = 0.0
        self.best_test_acc = 0.0
        self.best_epoch = -1
        self.best_model_state = None

        self.epoch_results = {}
        self.current_mask_sums = {}
        self.large_graph_builder = LargeGraphBuilder(
            num_neg_samples=config['large_graph']['num_neg_samples'],
            sampling_strategy=config['large_graph']['sampling_strategy'],
            random_seed=config['seed']
        )
        self.all_data = None
        self.all_labels = None        
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(reduction='none')
        self.lambda_l1 = config['train']['loss_weights']['lambda_l1']
        
        logger.info(f"âœ… Trainer initialized for Fold {fold}")
    
    def _build_models(self):
        """æ„å»ºæ¨¡å‹"""
        # ä¸»GNNæ¨¡å‹
        self.model = CausalNet(
            num_class=2,
            feature_dim=self.densenet_model.feature_dim,
            hidden1=self.config['model']['args']['hidden1'],
            hidden2=self.config['model']['args']['hidden2'],
            kernels=self.config['model']['args'].get('kernels', [2]),
            num_patches=self.edge_prior_mask.shape[0],
            num_neg_samples=self.config['large_graph']['num_neg_samples']
        ).to(self.device)
        
        # å› æœæ©ç æ¨¡å‹
        self.mask = CausalMask(
            num_patches=self.edge_prior_mask.shape[0], 
            edge_matrix=self.edge_prior_mask,    
            gumble_tau=self.config['misc']['gumble_tau']
        ).to(self.device)
        
        # DataParallel
        if torch.cuda.device_count() > 1:
            self.model = nn.DataParallel(self.model)
            self.mask = nn.DataParallel(self.mask)
        
        logger.info("âœ… Models built")
    
    def _setup_optimizers(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        # é¢„è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨ä¸DenseNetç›¸åŒçš„Adamé…ç½®
        self.optimizer_pretrain = optim.Adam(
            self.model.parameters(),
            lr=self.config['densenet']['pretrain']['learning_rate'],
            weight_decay=self.config['densenet']['pretrain']['weight_decay']
        )
        self.scheduler_pretrain = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer_pretrain,
            T_max=self.config['train']['pre_epoch'],
            eta_min=1e-5
        )

        # ä¸»è®­ç»ƒé˜¶æ®µï¼šä½¿ç”¨SGD
        self.optimizer = optim.SGD(
            self.model.parameters(),
            lr=self.config['train']['base_lr'],
            momentum=0.9,
            nesterov=True,
            weight_decay=self.config['train']['weight_decay']
        )
        self.optimizer_mask = optim.SGD(
            self.mask.parameters(),
            lr=self.config['train']['base_lr_mask'],
            momentum=0.9,
            nesterov=True,
            weight_decay=self.config['train']['weight_decay']
        )

        # ä¸»è®­ç»ƒå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.lr_scheduler = ReduceLROnPlateau(
            self.optimizer,
            verbose=(self.rank == 0),
            patience=self.config['train']['stepsize'],
            factor=self.config['train']['gamma']
        )
        self.lr_scheduler_mask = ReduceLROnPlateau(
            self.optimizer_mask,
            verbose=(self.rank == 0),
            patience=self.config['train']['stepsize'],
            factor=self.config['train']['gamma']
        )

        logger.info("âœ… Optimizers configured")
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader
    ) -> Dict[str, float]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸš€ Training Start - Fold {self.fold}")
        logger.info(f"{'='*80}\n")
        
        # é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º
        if self.all_data is None:
            logger.info("é¢„åŠ è½½æ•°æ®ç”¨äºå¤§å›¾æ„å»º...")
            dataset = train_loader.dataset
            if hasattr(dataset, 'dataset'):
                dataset = dataset.dataset
            self.all_data = np.array(dataset.all_patches)
            self.all_labels = np.array(dataset.labels)
            logger.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆ: {self.all_data.shape}")        
        
        # åˆå§‹åŒ–
        self._build_models()
        self._setup_optimizers()
        
        # é˜¶æ®µ1: é¢„è®­ç»ƒ (40 epochs)
        pre_epochs = self.config['train']['pre_epoch']
        if pre_epochs > 0:
            logger.info("\n" + "="*80)
            logger.info("ğŸ“š Phase 1: Pre-training (Whole Prediction)")
            logger.info("="*80)
            self._pretrain_phase(train_loader, val_loader, test_loader, pre_epochs)
        
        # é˜¶æ®µ2+3: ä¸»è®­ç»ƒ (100 epochs)
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ Phase 2+3: Main Training (Mask + GNN)")
        logger.info("="*80)
        self._main_training(train_loader, val_loader, test_loader)
        
        # æœ€ç»ˆè¯„ä¼°
        final_results = self._final_evaluation(test_loader)
        
        logger.info(f"\n{'='*80}")
        logger.info(f"âœ… Training Completed - Fold {self.fold}")
        logger.info(f"   Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"   Best Test Acc: {self.best_test_acc:.4f}")
        logger.info(f"{'='*80}\n")
        
        return final_results
    
    #==================== é¢„è®­ç»ƒé˜¶æ®µ ====================
    
    def _pretrain_phase(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: DataLoader,
        num_epochs: int
    ):
        """é¢„è®­ç»ƒé˜¶æ®µ"""
        for epoch in range(num_epochs):
            if self.rank == 0:
                self.epoch_results[epoch] = {}

            # è®­ç»ƒ
            self._train_pretrain_epoch(epoch, train_loader)

            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_pretrain_epoch(epoch, val_loader, 'val')
                    self._eval_pretrain_epoch(epoch, test_loader, 'test')

                self._print_pretrain_summary(epoch)

            # æ›´æ–°å­¦ä¹ ç‡ - æ·»åŠ è¿™è¡Œ
            self.scheduler_pretrain.step()

        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.rank == 0 and self.best_model_state:
            logger.info(f"âœ… Loading best pretrain model (Epoch {self.best_epoch+1})")
            if isinstance(self.model, nn.DataParallel):
                self.model.module.load_state_dict(self.best_model_state)
            else:
                self.model.load_state_dict(self.best_model_state)
    
    def _train_pretrain_epoch(self, epoch: int, train_loader: DataLoader):
        """é¢„è®­ç»ƒçš„ä¸€ä¸ªepoch"""
        self.model.train()
        
        losses = []
        accuracies = []
        
        for data, _, label in train_loader:
            self.global_step += 1
            
            # è½¬æ¢æ•°æ®
            data = data.to(self.device)
            label = label.to(self.device)
            
            # æå–ç‰¹å¾
            x_features = self._extract_features(data)
            
            # æ•´ä½“é¢„æµ‹
            outputs = self.model.module.prediction_whole(x_features, self.edge_prior_mask) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_prior_mask)
            
            loss = self.criterion(outputs, label).mean()
            l1_loss = self._compute_l1_regularization()
            loss_total = loss + l1_loss
            
            # åå‘ä¼ æ’­
            self.optimizer_pretrain.zero_grad()  # æ”¹è¿™é‡Œ
            loss_total.backward()
            self.optimizer_pretrain.step()  # æ”¹è¿™é‡Œ
            
            # è®°å½•
            if self.rank == 0:
                losses.append(loss.item())
                _, predicted = torch.max(outputs, 1)
                acc = (predicted == label).float().mean().item()
                accuracies.append(acc)
        
        # ä¿å­˜ç»“æœ
        if self.rank == 0:
            self.epoch_results[epoch]['train'] = {
                'loss_all': np.mean(losses),
                'acc_Intrinsic': np.mean(accuracies)
            }
    
    def _eval_pretrain_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        self.model.eval()

        all_outputs = []
        all_labels = []

        for data, _, label in data_loader:
            data = data.to(self.device)
            label = label.to(self.device)

            x_features = self._extract_features(data)
            outputs = self.model.module.prediction_whole(x_features, self.edge_prior_mask) \
                if isinstance(self.model, nn.DataParallel) else \
                self.model.prediction_whole(x_features, self.edge_prior_mask)

            all_outputs.append(outputs)
            all_labels.append(label)

        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)

        metrics = compute_binary_metrics(all_outputs, all_labels)
        self.epoch_results[epoch][phase] = metrics

        # âœ… ä½¿ç”¨é¢„è®­ç»ƒä¸“å±çš„æœ€ä½³æ¨¡å‹è¿½è¸ª
        if phase == 'val' and metrics['accuracy'] > self.pretrain_best_val_acc:
            self.pretrain_best_val_acc = metrics['accuracy']
            self.pretrain_best_epoch = epoch

            # ä¿å­˜é¢„è®­ç»ƒæœ€ä½³æ¨¡å‹
            if isinstance(self.model, nn.DataParallel):
                self.pretrain_best_model_state = self.model.module.state_dict()
            else:
                self.pretrain_best_model_state = self.model.state_dict()

            logger.info(f"ğŸ’ New Best Pretrain Val Acc: {metrics['accuracy']:.4f}, AUC: {metrics['auc']:.4f}")
    
    #==================== ä¸»è®­ç»ƒé˜¶æ®µ ====================
    def _main_training(self, train_loader, val_loader, test_loader):
        """ä¸»è®­ç»ƒé˜¶æ®µï¼ˆé˜¶æ®µ1+é˜¶æ®µ2ï¼‰"""
        start_epoch = self.config['train']['pre_epoch']
        num_epochs = self.config['train']['num_epoch']
        stage_transition = self.config['train']['stage_transition_epoch']

        # âœ… é˜¶æ®µ1çš„æœ€ä½³æ¨¡å‹è¿½è¸ª
        stage1_best_val_acc = 0.0
        stage1_best_model_state = None
        stage1_best_epoch = -1

        for epoch in range(start_epoch, num_epochs):
            is_stage1 = epoch < stage_transition

            if self.rank == 0:
                self.epoch_results[epoch] = {}

            # è®­ç»ƒ
            self._train_main_epoch(epoch, train_loader, is_stage1)

            # è¯„ä¼°
            if self.rank == 0:
                with torch.no_grad():
                    self._eval_main_epoch(epoch, val_loader, 'val')
                    self._eval_main_epoch(epoch, test_loader, 'test')

                # âœ… é˜¶æ®µ1ï¼šä¿å­˜é˜¶æ®µ1æœ€ä½³æ¨¡å‹
                if is_stage1:
                    val_acc = self.epoch_results[epoch]['val']['accuracy']
                    if val_acc > stage1_best_val_acc:
                        stage1_best_val_acc = val_acc
                        stage1_best_epoch = epoch
                        if isinstance(self.model, nn.DataParallel):
                            stage1_best_model_state = self.model.module.state_dict()
                        else:
                            stage1_best_model_state = self.model.state_dict()

                        logger.info(f"ğŸ’ New Best Stage1 Val Acc: {val_acc:.4f}")

                self._print_main_summary(epoch, is_stage1)

                # æ›´æ–°å­¦ä¹ ç‡
                if self.config['train']['scheduler'] == 'auto':
                    val_loss = self.epoch_results[epoch]['val'].get('gnn', {}).get('loss_all', 0)
                    self.lr_scheduler.step(val_loss)
                    self.lr_scheduler_mask.step(val_loss)
                else:
                    self.lr_scheduler.step()
                    self.lr_scheduler_mask.step()

            # âœ… é˜¶æ®µ1ç»“æŸ
            if epoch == stage_transition - 1 and self.rank == 0 and stage1_best_model_state:
                logger.info(f"\n{'='*80}")
                logger.info(f"âœ… Stage 1 Completed! Loading best Stage1 model as Stage2 starting point")
                logger.info(f"   Best Stage1 Epoch: {stage1_best_epoch + 1}")
                logger.info(f"   Best Stage1 Val Acc: {stage1_best_val_acc:.4f}")
                logger.info(f"{'='*80}\n")

                # é‡ç½®é˜¶æ®µ2çš„è¿½è¸ªå™¨
                self.best_val_acc = 0.0
                self.best_test_acc = 0.0
                self.best_epoch = -1

    def _train_main_epoch(self, epoch: int, train_loader: DataLoader, is_stage1: bool):
            """ä¸»è®­ç»ƒçš„ä¸€ä¸ªepoch (V3 - ä¿®å¤äº† 'int' object has no attribute 'item' é”™è¯¯)"""

            # ========== ã€æ–°ã€‘å®‰å…¨è·å–æŸå¤±å€¼çš„è¾…åŠ©å‡½æ•° ==========
            def _safe_get_item(loss_dict: dict, key: str) -> float:
                """
                å®‰å…¨åœ°ä»æŸå¤±å­—å…¸ä¸­è·å–å€¼å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ã€‚
                æ— è®ºå€¼æ˜¯ Tensor, int, è¿˜æ˜¯ float, éƒ½èƒ½å¤„ç†ã€‚
                """
                # è®¾ç½®ä¸€ä¸ªé»˜è®¤å¼ é‡ï¼Œä»¥é˜²keyä¸å­˜åœ¨
                default_tensor = torch.tensor(0.0, device=self.device)
                val = loss_dict.get(key, default_tensor)

                if isinstance(val, torch.Tensor):
                    return val.item() # å¦‚æœæ˜¯å¼ é‡ï¼Œè°ƒç”¨ .item()
                elif isinstance(val, (int, float)):
                    return float(val) # å¦‚æœæ˜¯ int/float (æ¯”å¦‚ 0)ï¼Œç›´æ¥è½¬æ¢
                else:
                    logger.warning(f"æ— æ³•è¯†åˆ«çš„æŸå¤±ç±»å‹ {type(val)} for key {key}. è¿”å› 0.0")
                    return 0.0
            # =======================================================

            self.model.train()
            self.mask.train()

            losses_mask = {
                'all': [], 'Intrinsic': [], 'Spurious': [], 'spurious_fusion': [],
                'intrinsic_fusion': [], 'sparsity_reg': []
            }
            losses_gnn = {
                'all': [], 'Intrinsic': [], 'spurious_fusion': [], 'l1_reg': []
            }
            accs_mask = {}
            accs_gnn = {}

            lambda_reg = 0.1 * (1 + epoch / self.config['train']['num_epoch'])
            stage_transition_epoch = self.config['train']['stage_transition_epoch']

            for batch_idx, (data, _, label) in enumerate(train_loader):

                is_first_batch_of_stage2 = (not is_stage1) and \
                                           (epoch == stage_transition_epoch) and \
                                           (batch_idx == 0)

                if is_first_batch_of_stage2:
                    logger.info(f"\n{'!'*80}")
                    logger.info(f"DEBUG [Epoch {epoch+1}, Batch {batch_idx}]: === æ­£åœ¨è¿›å…¥ Stage 2 çš„ç¬¬ä¸€ä¸ªæ‰¹æ¬¡ ===")
                    logger.info(f"{'!'*80}\n")

                self.global_step += 1
                label = label.to(self.device)

                # ============ æŒ‰é˜¶æ®µæ„å»ºæ•°æ®å’Œç‰¹å¾ ============
                if is_stage1:
                    data = data.to(self.device)
                    x_features = self._extract_features(data) # [B, P, feature_dim]
                else:
                    large_data, large_edge = self.large_graph_builder.build_large_graph(
                        batch_data=data,
                        batch_labels=label,
                        base_edge=self.edge_prior_mask.cpu(),
                        all_data=self.all_data,
                        all_labels=self.all_labels
                    )
                    large_data = large_data.to(self.device)
                    x_features = self._extract_features(large_data) # [B, (N+1)*P, feature_dim]

                    # ======== æ’æŸ¥ç‚¹ 1: æ£€æŸ¥ Stage 2 çš„è¾“å…¥ç‰¹å¾ (x_features) ========
                    if is_first_batch_of_stage2:
                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - x_features shape: {x_features.shape}")
                        if torch.isnan(x_features).any():
                            logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: x_features ä¸­æ£€æµ‹åˆ° NaN!")
                            raise ValueError("NaN found in features after _extract_features")
                        if torch.isinf(x_features).any():
                            logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: x_features ä¸­æ£€æµ‹åˆ° Inf!")
                            raise ValueError("Inf found in features after _extract_features")
                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: x_features (æ’æŸ¥ç‚¹ 1) æ£€æŸ¥é€šè¿‡ (æ—  NaN/Inf).")


                #========== 1. Maskè®­ç»ƒ ==========
                for param in self.model.parameters():
                    param.requires_grad = False

                mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
                masks, sparsity = mask_module(train=True)

                if is_stage1:
                    result_mask = self._compute_stage1_mask_loss(
                        x_features, masks, label, lambda_reg, self.edge_prior_mask, is_large_graph=False
                    )
                else:
                    result_mask = self._compute_stage2_mask_loss(
                        x_features, masks, label, lambda_reg, self.edge_prior_mask, is_large_graph=True
                    )

                    # ======== æ’æŸ¥ç‚¹ 2: æ£€æŸ¥ Stage 2 çš„ Mask æŸå¤±å€¼ (å·²ä¿®å¤) ========
                    if is_first_batch_of_stage2:
                        # ã€ä¿®å¤ã€‘: ä½¿ç”¨ _safe_get_item è¾…åŠ©å‡½æ•°
                        l_all = _safe_get_item(result_mask['loss'], 'all')
                        l_inv = _safe_get_item(result_mask['loss'], 'spurious_fusion')
                        l_sen = _safe_get_item(result_mask['loss'], 'intrinsic_fusion') # <-- ç°åœ¨è¿™é‡Œæ˜¯å®‰å…¨çš„
                        l_int = _safe_get_item(result_mask['loss'], 'Intrinsic')
                        l_spa = _safe_get_item(result_mask['loss'], 'sparsity_reg')

                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - Mask æŸå¤± (Backward ä¹‹å‰):")
                        logger.info(f"  â”œâ”€ Total Loss: {l_all:.4e}")
                        logger.info(f"  â”œâ”€ L_Intrinsic: {l_int:.4e}")
                        logger.info(f"  â”œâ”€ L_spurious_fusion (L_inv): {l_inv:.4e}")
                        logger.info(f"  â”œâ”€ L_intrinsic_fusion (L_sen): {l_sen:.4e}") # <-- é‡ç‚¹è§‚å¯Ÿå¯¹è±¡ (åº”ä¸º 0.0)
                        logger.info(f"  â””â”€ L_Sparsity: {l_spa:.4e}")

                        if not np.isfinite(l_all):
                            logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: Mask æŸå¤±å€¼ä¸º NaN/Infï¼")
                            raise ValueError("Mask loss exploded before backward")
                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Mask æŸå¤± (æ’æŸ¥ç‚¹ 2) æ£€æŸ¥é€šè¿‡ (æœ‰é™å€¼).")


                self.optimizer_mask.zero_grad()
                # åªæœ‰å½“ 'all' é”®å­˜åœ¨ä¸”æ˜¯å¼ é‡æ—¶æ‰è¿›è¡Œåå‘ä¼ æ’­
                if 'all' in result_mask['loss'] and isinstance(result_mask['loss']['all'], torch.Tensor):
                    result_mask['loss']['all'].backward()
                else:
                    logger.warning(f"DEBUG [E{epoch+1}, B{batch_idx}]: 'all' æŸå¤±ä¸æ˜¯å¼ é‡ï¼Œè·³è¿‡ Mask backwardã€‚")

                # ======== æ’æŸ¥ç‚¹ 3: æ£€æŸ¥ Stage 2 çš„ Mask æ¢¯åº¦ ========
                if is_first_batch_of_stage2:
                    max_grad_norm = 0.0
                    nan_grad = False
                    inf_grad = False
                    for param in self.mask.parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any(): nan_grad = True
                            if torch.isinf(param.grad).any(): inf_grad = True
                            current_norm = param.grad.data.norm(2).item()
                            if current_norm > max_grad_norm:
                                max_grad_norm = current_norm

                    logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - Mask æ¢¯åº¦ (Backward ä¹‹å):")
                    logger.info(f"  â”œâ”€ æœ€å¤§æ¢¯åº¦èŒƒæ•° (Max Grad Norm): {max_grad_norm:.4e}")

                    if nan_grad or inf_grad:
                        logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: Mask æ¢¯åº¦ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
                        raise ValueError("Mask gradients exploded (NaN/Inf)")
                    logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Mask æ¢¯åº¦ (æ’æŸ¥ç‚¹ 3) æ£€æŸ¥é€šè¿‡ (æ—  NaN/Inf).")

                self.optimizer_mask.step()

                # (è®°å½• Mask æŸå¤±)
                for k in losses_mask.keys():
                    losses_mask[k].append(_safe_get_item(result_mask['loss'], k)) # ã€ä¿®å¤ã€‘ä½¿ç”¨ safe_get
                for k, v in result_mask['preds'].items():
                    if k not in accs_mask: accs_mask[k] = []
                    accs_mask[k].append(self._compute_accuracy(v, label))


                #========== 2. GNNè®­ç»ƒ ==========
                for param in self.model.parameters():
                    param.requires_grad = True

                masks, sparsity = mask_module(train=False) # ä½¿ç”¨æ›´æ–°åçš„ Mask
                masks = [m.detach() for m in masks]

                if is_stage1:
                    result_gnn = self._compute_stage1_gnn_loss(
                        x_features, masks, label, self.edge_prior_mask, is_large_graph=False
                    )
                else:
                    result_gnn = self._compute_stage2_gnn_loss(
                        x_features, masks, label, self.edge_prior_mask, is_large_graph=True
                    )

                    # ======== æ’æŸ¥ç‚¹ 4: æ£€æŸ¥ Stage 2 çš„ GNN æŸå¤±å’ŒL1 (å·²ä¿®å¤) ========
                    if is_first_batch_of_stage2:
                        # ã€ä¿®å¤ã€‘: ä½¿ç”¨ _safe_get_item è¾…åŠ©å‡½æ•°
                        l1_reg_val = _safe_get_item(result_gnn['loss'], 'l1_reg')

                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - GNN L1 Reg (Backward ä¹‹å‰): {l1_reg_val:.4e}")

                        if not np.isfinite(l1_reg_val):
                             logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: GNN L1 Reg å€¼ä¸º NaN/Infï¼...")
                             raise ValueError("GNN weights (L1 Reg) exploded")

                        # ã€ä¿®å¤ã€‘: æ£€æŸ¥ GNN çš„æ€»æŸå¤±
                        l_all_gnn = _safe_get_item(result_gnn['loss'], 'all')
                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - GNN Total Loss (Backward ä¹‹å‰): {l_all_gnn:.4e}")

                        if not np.isfinite(l_all_gnn):
                            logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: GNN æŸå¤±å€¼ä¸º NaN/Infï¼(Mask æŸåå¯¼è‡´)")
                            raise ValueError("GNN loss exploded (likely due to corrupted mask)")

                        logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: GNN æŸå¤± (æ’æŸ¥ç‚¹ 4) æ£€æŸ¥é€šè¿‡ã€‚")


                # åªæœ‰å½“ 'all' é”®å­˜åœ¨ä¸”æ˜¯å¼ é‡æ—¶æ‰è¿›è¡Œåå‘ä¼ æ’­
                if 'all' in result_gnn['loss'] and isinstance(result_gnn['loss']['all'], torch.Tensor):
                    result_gnn['loss']['all'].backward()
                else:
                    logger.warning(f"DEBUG [E{epoch+1}, B{batch_idx}]: 'all' æŸå¤±ä¸æ˜¯å¼ é‡ï¼Œè·³è¿‡ GNN backwardã€‚")

                # ======== æ’æŸ¥ç‚¹ 5: æ£€æŸ¥ Stage 2 çš„ GNN æ¢¯åº¦ ========
                if is_first_batch_of_stage2:
                    max_grad_norm_gnn = 0.0
                    nan_grad_gnn = False
                    inf_grad_gnn = False
                    for param in self.model.parameters():
                        if param.grad is not None:
                            if torch.isnan(param.grad).any(): nan_grad_gnn = True
                            if torch.isinf(param.grad).any(): inf_grad_gnn = True
                            current_norm = param.grad.data.norm(2).item()
                            if current_norm > max_grad_norm_gnn:
                                max_grad_norm_gnn = current_norm

                    logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: Stage 2 - GNN æ¢¯åº¦ (Backward ä¹‹å):")
                    logger.info(f"  â”œâ”€ æœ€å¤§æ¢¯åº¦èŒƒæ•° (Max Grad Norm): {max_grad_norm_gnn:.4e}")

                    if nan_grad_gnn or inf_grad_gnn:
                        logger.error(f"FATAL [E{epoch+1}, B{batch_idx}]: GNN æ¢¯åº¦ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
                        raise ValueError("GNN gradients exploded (NaN/Inf)")
                    logger.info(f"DEBUG [E{epoch+1}, B{batch_idx}]: GNN æ¢¯åº¦ (æ’æŸ¥ç‚¹ 5) æ£€æŸ¥é€šè¿‡ (æ—  NaN/Inf).")

                self.optimizer.step()

                # (è®°å½• GNN æŸå¤±)
                for k in losses_gnn.keys():
                    losses_gnn[k].append(_safe_get_item(result_gnn['loss'], k)) # ã€ä¿®å¤ã€‘ä½¿ç”¨ safe_get
                for k, v in result_gnn['preds'].items():
                    if k not in accs_gnn: accs_gnn[k] = []
                    accs_gnn[k].append(self._compute_accuracy(v, label))

                if self.rank == 0:
                    self.current_mask_sums = {
                        'node': masks[0].sum().item(),
                        'edge': masks[1].sum().item()
                    }

            # ============ Epoch ç»“æŸï¼Œä¿å­˜ç»“æœ ============
            if self.rank == 0:
                train_res = {'mask': {}, 'gnn': {}}

                for k in losses_mask.keys():
                    train_res['mask'][k] = float(np.mean(losses_mask[k])) if len(losses_mask[k]) > 0 else 0.0
                for k in losses_gnn.keys():
                    train_res['gnn'][k] = float(np.mean(losses_gnn[k])) if len(losses_gnn[k]) > 0 else 0.0
                for k, v in accs_mask.items():
                    train_res['mask'][f'acc_{k}'] = float(np.mean(v)) if len(v) > 0 else 0.0
                for k, v in accs_gnn.items():
                    train_res['gnn'][f'acc_{k}'] = float(np.mean(v)) if len(v) > 0 else 0.0

                self.epoch_results[epoch]['train'] = train_res
    
    def _eval_main_epoch(self, epoch: int, data_loader: DataLoader, phase: str):
        """ä¸»è®­ç»ƒè¯„ä¼°"""
        self.model.eval()
        self.mask.eval()

        all_outputs = []
        all_labels = []

        for data, _, label in data_loader:
            data = data.to(self.device)
            label = label.to(self.device)

            x_features = self._extract_features(data)
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            masks, probs, sparsity = mask_module(train=False, return_probs=True)

            model_module = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
            outputs = model_module.prediction_intrinsic_path(x_features, self.edge_prior_mask, masks, is_large_graph=False)

            all_outputs.append(outputs)
            all_labels.append(label)

        all_outputs = torch.cat(all_outputs, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        metrics = compute_binary_metrics(all_outputs, all_labels)
        self.epoch_results[epoch][phase] = metrics

        # âœ… ä»…åœ¨é˜¶æ®µ2æ›´æ–°æœ€ç»ˆè¯„ä¼°çš„æœ€ä½³æ¨¡å‹
        stage_transition = self.config['train']['stage_transition_epoch']
        if phase == 'val' and epoch >= stage_transition and metrics['accuracy'] > self.best_val_acc:
            self.best_val_acc = metrics['accuracy']
            self.best_epoch = epoch

            if 'test' in self.epoch_results[epoch]:
                self.best_test_acc = self.epoch_results[epoch]['test']['accuracy']
                logger.info(
                    f"ğŸ’ New Best Stage2 - Val Acc: {metrics['accuracy']:.4f}, "
                    f"Val AUC: {metrics['auc']:.4f}, "
                    f"Test Acc: {self.best_test_acc:.4f}"
                )
            else:
                logger.info(
                    f"ğŸ’ New Best Stage2 Val - Acc: {metrics['accuracy']:.4f}, "
                    f"AUC: {metrics['auc']:.4f}, F1: {metrics['f1']:.4f}"
                )
                
    
    #==================== æŸå¤±è®¡ç®— ====================
    
    def _compute_stage1_mask_loss(self, x, masks, label, lambda_reg, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ1 MaskæŸå¤±ï¼šå†…åœ¨å­å›¾ + è™šå‡å­å›¾"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # å†…åœ¨å­å›¾
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        # è™šå‡å­å›¾ï¼ˆç†µæŸå¤±ï¼‰
        y_spu = model.prediction_spurious_path(x, edge_prior_mask, masks, is_large_graph)
        loss_spu = self._entropy_loss(y_spu)
        
        # ç¨€ç–æ€§æ­£åˆ™
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(lambda_reg=lambda_reg)
        
        # æ€»æŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = loss_weights['L_pred'] * loss_pred + loss_weights['L_spu'] * loss_spu + reg_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'Spurious': loss_spu,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'Intrinsic': y_pred,
                'Spurious': y_spu
            }
        }
    
    def _compute_stage2_mask_loss(self, x, masks, label, lambda_reg, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ2 MaskæŸå¤±ï¼šè™šå‡å­å›¾å¹²æ‰° + å†…åœ¨å­å›¾å¹²æ‰° + å†…åœ¨å­å›¾"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        # å†…åœ¨å­å›¾
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        # è™šå‡å­å›¾å¹²æ‰°æ€§
        y_inv = model.prediction_spurious_fusion(x, edge_prior_mask, masks, is_large_graph)   # ä½¿ç”¨ç›¸åŒæ–¹æ³•
        loss_inv = self.criterion(y_inv, label).mean()
        
        # å†…åœ¨å­å›¾å¹²æ‰°
        y_sen = model.prediction_intrinsic_fusion(x, edge_prior_mask, masks, is_large_graph)
        loss_sen = self.criterion(y_sen, 1 - label).mean()
        # ç¨€ç–æ€§æ­£åˆ™
        mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
        reg_loss = mask_module.compute_sparsity_regularization(lambda_reg=lambda_reg)
        
        # æ€»æŸå¤±
        loss_weights = self.config['train']['loss_weights']
        loss_all = (loss_weights['L_inv'] * loss_inv + 
                   loss_weights['L_sen'] * loss_sen + 
                   loss_weights['L_pred'] * loss_pred + 
                   reg_loss)
        
        return {
            'loss': {
                'all': loss_all,
                'spurious_fusion': loss_inv,
                'intrinsic_fusion': loss_sen,
                'Intrinsic': loss_pred,
                'sparsity_reg': reg_loss
            },
            'preds': {
                'spurious_fusion': y_inv,
                'intrinsic_fusion': y_sen,
                'Intrinsic': y_pred
            }
        }
    def _compute_stage1_gnn_loss(self, x, masks, label, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ1 GNNæŸå¤±ï¼šå†…åœ¨å­å›¾"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_pred + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'l1_reg': l1_loss
            },
            'preds': {
                'Intrinsic': y_pred
            }
        }
    
    def _compute_stage2_gnn_loss(self, x, masks, label, edge_prior_mask, is_large_graph):
        """é˜¶æ®µ2 GNNæŸå¤±ï¼šå†…åœ¨å­å›¾ + è™šå‡å­å›¾å¹²æ‰°"""
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        y_pred = model.prediction_intrinsic_path(x, edge_prior_mask, masks, is_large_graph)
        loss_pred = self.criterion(y_pred, label).mean()
        
        y_inv = model.prediction_spurious_fusion(x, edge_prior_mask, masks, is_large_graph) 
        loss_inv = self.criterion(y_inv, label).mean()
        
        l1_loss = self._compute_l1_regularization()
        loss_all = loss_pred + loss_inv + l1_loss
        
        return {
            'loss': {
                'all': loss_all,
                'Intrinsic': loss_pred,
                'spurious_fusion': loss_inv,
                'l1_reg': l1_loss
            },
            'preds': {
                'Intrinsic': y_pred,
                'spurious_fusion': y_inv
            }
        }
    
    #==================== è¾…åŠ©å‡½æ•° ====================
    
    def _extract_features(self, data: torch.Tensor, batch_size: int = 32) -> torch.Tensor:
        """ä½¿ç”¨DenseNetæ‰¹é‡æå–ç‰¹å¾ï¼ˆé¿å…OOMï¼‰"""
        B = data.shape[0]
        total_P = data.shape[1]

        data_reshaped = data.view(-1, 1, data.shape[3], data.shape[4], data.shape[5])
        total_patches = data_reshaped.shape[0]

        # æ‰¹é‡æå–ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸
        all_features = []
        with torch.no_grad():
            for i in range(0, total_patches, batch_size):
                batch = data_reshaped[i:i+batch_size]
                features_batch = self.densenet_model(batch)
                all_features.append(features_batch.cpu())  # ç«‹å³ç§»åˆ°CPU
                del features_batch
                torch.cuda.empty_cache()

        # åœ¨CPUä¸Šæ‹¼æ¥ï¼Œå†ç§»å›GPU
        features = torch.cat(all_features, dim=0).to(self.device)
        features = features.view(B, total_P, -1)
        return features
    
    def _compute_l1_regularization(self) -> torch.Tensor:
        """è®¡ç®—L1æ­£åˆ™åŒ–"""
        l1_reg = torch.tensor(0., device=self.device)
        model = self.model.module if isinstance(self.model, nn.DataParallel) else self.model
        
        for name, param in model.named_parameters():
            if 'mlp_causal.0.weight' in name:
                l1_reg += torch.sum(torch.abs(param))
        
        return self.lambda_l1 * l1_reg
    
    def _entropy_loss(self, logits: torch.Tensor) -> torch.Tensor:
        """ç†µæŸå¤±ï¼ˆç”¨äºè™šå‡å­å›¾ï¼‰"""
        probs = torch.softmax(logits, dim=1)
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=1)
        return -entropy.mean()  # è´Ÿå·ä½¿å…¶æœ€å¤§åŒ–ç†µ
    
    def _compute_accuracy(self, outputs: torch.Tensor, labels: torch.Tensor) -> float:
        """è®¡ç®—å‡†ç¡®ç‡"""
        _, predicted = torch.max(outputs, 1)
        acc = (predicted == labels).float().mean().item()
        return acc
    
    #==================== æ‰“å° ====================
    
    def _print_pretrain_summary(self, epoch: int):
        """æ‰“å°é¢„è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})

        logger.info("="*80)
        logger.info(f"ğŸ“š Epoch {epoch+1}/{self.config['train']['pre_epoch']} [Pre-train]")
        logger.info("-"*80)
        logger.info(f"Train - Loss: {train_res.get('loss_all', 0):.4f}, "
                   f"Acc: {train_res.get('acc_Intrinsic', 0):.4f}")

        # âœ… å¢å¼ºéªŒè¯é›†æ—¥å¿—
        logger.info(f"Val   - Acc: {val_res.get('accuracy', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}, "
                   f"F1: {val_res.get('f1', 0):.4f}")
        logger.info(f"        Sens: {val_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {val_res.get('specificity', 0):.4f}")

        # âœ… å¢å¼ºæµ‹è¯•é›†æ—¥å¿—
        logger.info(f"Test  - Acc: {test_res.get('accuracy', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}, "
                   f"F1: {test_res.get('f1', 0):.4f}")
        logger.info(f"        Sens: {test_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {test_res.get('specificity', 0):.4f}")

        logger.info("="*80 + "\n")
    

    def _print_main_summary(self, epoch: int, is_stage1: bool):
        """æ‰“å°ä¸»è®­ç»ƒæ€»ç»“"""
        res = self.epoch_results[epoch]
        train_res = res.get('train', {})
        val_res = res.get('val', {})
        test_res = res.get('test', {})

        mask_res = train_res.get('mask', {})
        gnn_res = train_res.get('gnn', {})

        stage_name = "Stage 1 (Intrinsic+Spurious)" if is_stage1 else "Stage 2 (spurious_fusion+intrinsic_fusion)"

        logger.info("="*80)
        logger.info(f"ğŸ¯ Epoch {epoch+1}/{self.config['train']['num_epoch']} [{stage_name}]")
        logger.info("-"*80)

        # âœ… å¢å¼ºå®˜æ–¹è¯„ä¼°ç»“æœ
        logger.info(f"ğŸ“Š Validation Metrics:")
        logger.info(f"   Acc: {val_res.get('accuracy', 0):.4f}, "
                   f"AUC: {val_res.get('auc', 0):.4f}, "
                   f"F1: {val_res.get('f1', 0):.4f}")
        logger.info(f"   Sens: {val_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {val_res.get('specificity', 0):.4f}, "
                   f"Prec: {val_res.get('precision', 0):.4f}")

        logger.info(f"\nğŸ“Š Test Metrics:")
        logger.info(f"   Acc: {test_res.get('accuracy', 0):.4f}, "
                   f"AUC: {test_res.get('auc', 0):.4f}, "
                   f"F1: {test_res.get('f1', 0):.4f}")
        logger.info(f"   Sens: {test_res.get('sensitivity', 0):.4f}, "
                   f"Spec: {test_res.get('specificity', 0):.4f}, "
                   f"Prec: {test_res.get('precision', 0):.4f}")

        # âœ… ä¿®å¤ Mask è®­ç»ƒè¯¦æƒ…ï¼ˆé”®ååŒ¹é…ï¼‰
        logger.info(f"\nğŸ­ Mask Training:")
        logger.info(f"   Total Loss: {mask_res.get('all', 0):.4f}")  # âœ… æ”¹ä¸º 'all'
        if is_stage1:
            logger.info(f"     â”œâ”€ Intrinsic:  {mask_res.get('Intrinsic', 0):.4f} "  # âœ… æ”¹ä¸º 'Intrinsic'
                       f"(Acc: {mask_res.get('acc_Intrinsic', 0):.2%})")
            logger.info(f"     â”œâ”€ Spurious: {mask_res.get('Spurious', 0):.4f} "  # âœ… æ”¹ä¸º 'Spurious'
                       f"(Acc: {mask_res.get('acc_Spurious', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:    {mask_res.get('sparsity_reg', 0):.4f}")  # âœ… æ”¹ä¸º 'sparsity_reg'
        else:
            logger.info(f"     â”œâ”€ Intrinsic:     {mask_res.get('Intrinsic', 0):.4f} "  # âœ…
                       f"(Acc: {mask_res.get('acc_Intrinsic', 0):.2%})")
            logger.info(f"     â”œâ”€ spurious_fusion:         {mask_res.get('spurious_fusion', 0):.4f} "  # âœ… æ”¹ä¸º 'spurious_fusion'
                       f"(Acc: {mask_res.get('acc_causal', 0):.2%})")
            logger.info(f"     â”œâ”€ intrinsic_fusion: {mask_res.get('intrinsic_fusion', 0):.4f} "  # âœ… æ”¹ä¸º 'intrinsic_fusion'
                       f"(Acc: {mask_res.get('acc_intrinsic_fusion', 0):.2%})")
            logger.info(f"     â””â”€ Sparsity:       {mask_res.get('sparsity_reg', 0):.4f}")  # âœ…

        # âœ… ä¿®å¤ GNN è®­ç»ƒè¯¦æƒ…ï¼ˆé”®ååŒ¹é…ï¼‰
        logger.info(f"\nğŸ§  GNN Training:")
        logger.info(f"   Total Loss: {gnn_res.get('all', 0):.4f}")  # âœ… æ”¹ä¸º 'all'
        logger.info(f"     â”œâ”€ Intrinsic: {gnn_res.get('Intrinsic', 0):.4f} "  # âœ… æ”¹ä¸º 'Intrinsic'
                   f"(Acc: {gnn_res.get('acc_Intrinsic', 0):.2%})")
        if not is_stage1:
            logger.info(f"     â”œâ”€ spurious_fusion:     {gnn_res.get('spurious_fusion', 0):.4f} "  # âœ… æ”¹ä¸º 'spurious_fusion'
                       f"(Acc: {gnn_res.get('acc_causal', 0):.2%})")
        logger.info(f"     â””â”€ L1 Reg:     {gnn_res.get('l1_reg', 0):.4f}")  # âœ… æ”¹ä¸º 'l1_reg'

        # è®­ç»ƒå‚æ•°
        lr_gnn = self.optimizer.param_groups[0]['lr']
        lr_mask = self.optimizer_mask.param_groups[0]['lr']
        logger.info(f"\nâš™ï¸  Learning Rates:")
        logger.info(f"   GNN:  {lr_gnn:.6f}")
        logger.info(f"   Mask: {lr_mask:.6f}")

        # æ©ç ç»Ÿè®¡
        if self.current_mask_sums:
            mask_module = self.mask.module if isinstance(self.mask, nn.DataParallel) else self.mask
            total_nodes = mask_module.P
            total_edges = int(mask_module.learnable_mask.sum().item())

            node_sum = int(self.current_mask_sums.get('node', 0))
            edge_sum = int(self.current_mask_sums.get('edge', 0))

            node_pct = node_sum / total_nodes * 100 if total_nodes > 0 else 0
            edge_pct = edge_sum / total_edges * 100 if total_edges > 0 else 0

            logger.info(f"\nğŸ­ Mask Statistics:")
            logger.info(f"   Nodes: {node_sum}/{total_nodes} ({node_pct:.1f}%)")
            logger.info(f"   Edges: {edge_sum}/{total_edges} ({edge_pct:.1f}%)")

        logger.info("="*80 + "\n")
    
    def _final_evaluation(self, test_loader: DataLoader) -> Dict[str, float]:
        """æœ€ç»ˆè¯„ä¼°"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ Final Evaluation")
        logger.info("="*80)

        # âœ… å¦‚æœ best_test_acc ä»ç„¶æ˜¯ 0ï¼Œè¯´æ˜æ²¡æœ‰æ­£ç¡®è®°å½•
        # è¿™æ—¶åº”è¯¥ä» epoch_results ä¸­è·å–æœ€ä½³ epoch å¯¹åº”çš„æµ‹è¯•é›†å‡†ç¡®ç‡
        if self.best_test_acc == 0.0 and self.best_epoch >= 0:
            if 'test' in self.epoch_results.get(self.best_epoch, {}):
                self.best_test_acc = self.epoch_results[self.best_epoch]['test']['accuracy']
                logger.info(f"â„¹ï¸  Retrieved test acc from epoch {self.best_epoch + 1}")

        results = {
            'fold': self.fold,
            'best_epoch': self.best_epoch,
            'val_acc': self.best_val_acc,
            'test_acc': self.best_test_acc
        }

        logger.info(f"Best Epoch:    {self.best_epoch + 1}")
        logger.info(f"Best Val Acc:  {self.best_val_acc:.4f}")
        logger.info(f"Best Test Acc: {self.best_test_acc:.4f}")
        logger.info("="*80)

        return results