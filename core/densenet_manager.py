"""
DenseNet Pretraining Manager
DenseNeté¢„è®­ç»ƒè‡ªåŠ¨åŒ–ç®¡ç†å™¨
"""

import os
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from typing import Dict, Any, Optional, Tuple
from pathlib import Path

from utils.checkpoint import CheckpointManager
from models.densenet import LightDenseNet3D, EndToEndDenseNet
from data.dataset import PatchDataset, collate_fn, get_fold_splits


logger = logging.getLogger(__name__)


class DenseNetManager:
    """
    DenseNeté¢„è®­ç»ƒç®¡ç†å™¨
    
    åŠŸèƒ½ï¼š
        1. è‡ªåŠ¨æ£€æµ‹ç¼“å­˜çš„é¢„è®­ç»ƒæ¨¡å‹
        2. æŒ‰éœ€è§¦å‘è®­ç»ƒï¼ˆä»…åœ¨ç¼“å­˜ä¸å­˜åœ¨æ—¶ï¼‰
        3. æä¾›ç»Ÿä¸€çš„æ¨¡å‹åŠ è½½æ¥å£
        4. ç¡®ä¿ä¸ä¸»è®­ç»ƒæµç¨‹çš„æ•°æ®åˆ†å‰²ä¸€è‡´æ€§
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        checkpoint_manager: ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
        config: è®­ç»ƒé…ç½®å­—å…¸
    """
    
    def __init__(
        self,
        data_dir: str,
        checkpoint_manager: CheckpointManager,
        config: Optional[Dict[str, Any]] = None
    ):
        self.data_dir = data_dir
        self.checkpoint_manager = checkpoint_manager
        
        # é»˜è®¤é…ç½®
        self.config = {
            'growth_rate': 8,
            'num_init_features': 24,
            'num_epochs': 50,
            'stage1_epochs': 49,  # é˜¶æ®µ1è½®æ•°ï¼ˆæ— maskï¼‰
            'mask_ratio': 0.3,     # é˜¶æ®µ2çš„maskæ¯”ä¾‹
            'batch_size': 16,
            'learning_rate_stage1': 0.0001,
            'learning_rate_stage2': 0.0004,
            'weight_decay_stage1': 1e-4,
            'weight_decay_stage2': 1e-5,
            'num_workers': 4,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }
        
        # æ›´æ–°ç”¨æˆ·æä¾›çš„é…ç½®
        if config:
            self.config.update(config)
        
        logger.info(f"DenseNetManager initialized with data_dir: {data_dir}")
        logger.info(f"Training config: {self.config}")
    
    def get_pretrained_model(
        self,
        fold: int,
        split_seed: int,
        force_retrain: bool = False
    ) -> LightDenseNet3D:
        """
        è·å–é¢„è®­ç»ƒçš„DenseNetæ¨¡å‹
        
        è‡ªåŠ¨æ£€æµ‹ç¼“å­˜ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è§¦å‘è®­ç»ƒ
        
        Args:
            fold: foldç´¢å¼•
            split_seed: æ•°æ®åˆ†å‰²ç§å­
            force_retrain: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            é¢„è®­ç»ƒå¥½çš„LightDenseNet3Dæ¨¡å‹
        """
        # æ„å»ºç¼“å­˜æ ‡è¯†ç¬¦
        config_params = {
            'growth_rate': self.config['growth_rate'],
            'num_init_features': self.config['num_init_features'],
            'num_epochs': self.config['num_epochs'],
            'stage1_epochs': self.config['stage1_epochs'],
            'mask_ratio': self.config['mask_ratio']
        }
        
        identifier = self.checkpoint_manager.build_identifier(
            'densenet',
            config_params,
            {'fold': fold, 'seed': split_seed}
        )
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        cache_exists = self.checkpoint_manager.check_exists('densenet', identifier)
        
        if cache_exists and not force_retrain:
            logger.info(f"[Fold {fold}] Loading cached pretrained DenseNet: {identifier}")
            return self._load_pretrained_model(identifier)
        else:
            if force_retrain:
                logger.info(f"[Fold {fold}] Force retraining DenseNet...")
            else:
                logger.info(f"[Fold {fold}] No cache found, starting training...")
            
            return self._train_and_save(fold, split_seed, identifier, config_params)
    
    def _load_pretrained_model(self, identifier: str) -> LightDenseNet3D:
        """ä»ç¼“å­˜åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        checkpoint = self.checkpoint_manager.load(
            'densenet',
            identifier,
            map_location='cpu'
        )
        
        # åˆ›å»ºæ¨¡å‹å¹¶åŠ è½½æƒé‡
        model = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        
        # checkpointå¯èƒ½æ˜¯å®Œæ•´çš„å­—å…¸æˆ–åªæ˜¯state_dict
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"Loaded from epoch {checkpoint.get('epoch', 'unknown')}, "
                       f"val_acc: {checkpoint.get('val_acc', 'unknown'):.4f}")
        else:
            model.load_state_dict(checkpoint)
        
        logger.info(f"Successfully loaded pretrained DenseNet")
        return model
    
    def _train_and_save(
        self,
        fold: int,
        split_seed: int,
        identifier: str,
        config_params: Dict[str, Any]
    ) -> LightDenseNet3D:
        """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹"""
        
        logger.info(f"\n{'='*70}")
        logger.info(f"Starting DenseNet Pretraining - Fold {fold}")
        logger.info(f"{'='*70}\n")
        
        # 1. å‡†å¤‡æ•°æ®
        train_loader, val_loader = self._prepare_dataloaders(fold, split_seed)
        
        # 2. åˆ›å»ºæ¨¡å‹
        device = torch.device(self.config['device'])
        
        feature_extractor = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        
        # è·å–patchæ•°é‡
        dataset = PatchDataset(self.data_dir)
        num_patches = dataset.get_num_patches()
        
        model = EndToEndDenseNet(feature_extractor, num_patches).to(device)
        
        # å¤šGPUæ”¯æŒ
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            logger.info(f"Using {torch.cuda.device_count()} GPUs with DataParallel")
            model = nn.DataParallel(model)
        
        # 3. æ‰§è¡Œä¸¤é˜¶æ®µè®­ç»ƒ
        best_model_path = self._two_stage_training(
            model, train_loader, val_loader, device, fold
        )
        
        # 4. ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°ç¼“å­˜
        logger.info(f"Saving best model to cache...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_checkpoint = torch.load(best_model_path, map_location='cpu')
        
        # æå–feature_extractorçš„æƒé‡
        if isinstance(model, nn.DataParallel):
            feature_extractor_state = model.module.feature_extractor.state_dict()
        else:
            feature_extractor_state = model.feature_extractor.state_dict()
        
        # ä¿å­˜åˆ°ç¼“å­˜ç³»ç»Ÿ
        save_data = {
            'model_state_dict': feature_extractor_state,
            'epoch': best_checkpoint['epoch'],
            'val_acc': best_checkpoint['val_acc'],
            'val_loss': best_checkpoint['val_loss']
        }
        
        self.checkpoint_manager.save(
            'densenet',
            identifier,
            save_data,
            config_params
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(best_model_path):
            os.remove(best_model_path)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"DenseNet Pretraining Completed - Fold {fold}")
        logger.info(f"Best Val Acc: {best_checkpoint['val_acc']:.4f}")
        logger.info(f"{'='*70}\n")
        
        # è¿”å›çº¯å‡€çš„feature_extractor
        clean_feature_extractor = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        clean_feature_extractor.load_state_dict(feature_extractor_state)
        
        return clean_feature_extractor
    
    def _prepare_dataloaders(
        self,
        fold: int,
        split_seed: int
    ) -> Tuple[DataLoader, DataLoader]:
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨"""
        
        # åŠ è½½å®Œæ•´æ•°æ®é›†
        dataset = PatchDataset(self.data_dir)
        
        # è·å–æ•°æ®åˆ†å‰²ç´¢å¼•ï¼ˆä¸ä¸»è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        train_indices, val_indices, _ = get_fold_splits(
            self.data_dir, fold, split_seed
        )
        
        logger.info(f"Data split - Train: {len(train_indices)}, Val: {len(val_indices)}")
        
        # åˆ›å»ºå­é›†
        train_subset = Subset(dataset, train_indices)
        val_subset = Subset(dataset, val_indices)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_subset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_subset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    def _two_stage_training(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        device: torch.device,
        fold: int
    ) -> str:
        """
        ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥
        
        Stage 1: MLPåŸºç¡€è®­ç»ƒï¼ˆæ— maskï¼‰
        Stage 2: MLP + Maskè®­ç»ƒï¼ˆæ¿€è¿›maskï¼‰
        
        Returns:
            æœ€ä½³æ¨¡å‹çš„ä¸´æ—¶ä¿å­˜è·¯å¾„
        """
        criterion = nn.CrossEntropyLoss()
        
        stage1_epochs = self.config['stage1_epochs']
        total_epochs = self.config['num_epochs']
        stage2_epochs = total_epochs - stage1_epochs
        
        best_val_acc = 0.0
        best_val_loss = float('inf')
        best_epoch = -1
        
        # ä¸´æ—¶ä¿å­˜è·¯å¾„
        temp_save_path = f'./temp_densenet_fold{fold}_best.pth'
        
        logger.info(f"Stage 1: Epochs 1-{stage1_epochs} (MLPåŸºç¡€è®­ç»ƒ, no mask)")
        logger.info(f"Stage 2: Epochs {stage1_epochs+1}-{total_epochs} (MLP+Maskè®­ç»ƒ)")
        
        optimizer = None
        scheduler = None
        
        # è·å–å®é™…çš„æ¨¡å‹ï¼ˆå¤„ç†DataParallelåŒ…è£…ï¼‰
        actual_model = model.module if isinstance(model, nn.DataParallel) else model
        
        for epoch in range(total_epochs):
            # åŠ¨æ€è°ƒæ•´è®­ç»ƒç­–ç•¥
            if epoch < stage1_epochs:
                # Stage 1: æ— maskè®­ç»ƒ
                if epoch == 0:
                    logger.info(f"\n{'='*50}")
                    logger.info("Entering Stage 1: MLPåŸºç¡€è®­ç»ƒ")
                    logger.info(f"{'='*50}\n")
                    optimizer = optim.Adam(
                        model.parameters(),
                        lr=self.config['learning_rate_stage1'],
                        weight_decay=self.config['weight_decay_stage1']
                    )
                    scheduler = optim.lr_scheduler.CosineAnnealingLR(
                        optimizer, T_max=stage1_epochs, eta_min=1e-5
                    )
                actual_model.mask_ratio = 0.0
                stage_info = "Stage1(No Mask)"
            else:
                # Stage 2: æ¿€è¿›maskè®­ç»ƒ
                if epoch == stage1_epochs:
                    logger.info(f"\n{'='*50}")
                    logger.info("Entering Stage 2: MLP+Maskè®­ç»ƒ")
                    logger.info(f"{'='*50}\n")
                    optimizer = optim.Adam(
                        model.parameters(),
                        lr=self.config['learning_rate_stage2'],
                        weight_decay=self.config['weight_decay_stage2']
                    )
                    scheduler = optim.lr_scheduler.CosineAnnealingLR(
                        optimizer, T_max=stage2_epochs, eta_min=1e-6
                    )
                actual_model.mask_ratio = self.config['mask_ratio']
                stage_info = f"Stage2(Mask={self.config['mask_ratio']:.0%})"
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss, train_acc = self._train_one_epoch(
                model, train_loader, criterion, optimizer, device, actual_model.mask_ratio
            )
            
            # éªŒè¯
            val_loss, val_acc = self._validate(
                model, val_loader, criterion, device
            )
            
            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):
                best_val_acc = val_acc
                best_val_loss = val_loss
                best_epoch = epoch
                
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': actual_model.feature_extractor.state_dict(),
                    'val_acc': val_acc,
                    'val_loss': val_loss
                }, temp_save_path)
                
                logger.info(f"ğŸ’ [New Best] Epoch {epoch+1}: Val Acc={val_acc:.4f}, Val Loss={val_loss:.4f}")
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            logger.info(
                f"Epoch [{epoch+1}/{total_epochs}] {stage_info} | "
                f"LR: {current_lr:.1e} | "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}"
            )
        
        logger.info(f"\nBest model from Epoch {best_epoch+1}: "
                   f"Val Acc={best_val_acc:.4f}, Val Loss={best_val_loss:.4f}")
        
        return temp_save_path
    
    def _train_one_epoch(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: torch.device,
        mask_ratio: float
    ) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        for patches, _, labels in train_loader:
            patches = patches.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(patches)
            loss = criterion(outputs, labels)
            
            # å¦‚æœä½¿ç”¨maskï¼Œæ·»åŠ ä¸€è‡´æ€§æŸå¤±
            if mask_ratio > 0:
                outputs_no_mask = self._forward_no_mask(model, patches)
                consistency_loss = F.kl_div(
                    F.log_softmax(outputs, dim=1),
                    F.softmax(outputs_no_mask, dim=1),
                    reduction='batchmean'
                )
                loss = loss + 0.2 * consistency_loss
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def _validate(
        self,
        model: nn.Module,
        val_loader: DataLoader,
        criterion: nn.Module,
        device: torch.device
    ) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹"""
        model.eval()
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for patches, _, labels in val_loader:
                patches = patches.to(device)
                labels = labels.to(device)
                
                # éªŒè¯æ—¶ä¸ä½¿ç”¨mask
                outputs = self._forward_no_mask(model, patches)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    def _forward_no_mask(self, model: nn.Module, patches: torch.Tensor) -> torch.Tensor:
        """
        æ— maskçš„å‰å‘ä¼ æ’­ï¼ˆç”¨äºéªŒè¯å’Œä¸€è‡´æ€§æŸå¤±ï¼‰
        
        å¤„ç†DataParallelçš„ç‰¹æ®Šæƒ…å†µ
        """
        if isinstance(model, nn.DataParallel):
            # DataParallelæƒ…å†µï¼šä¸´æ—¶åˆ‡æ¢åˆ°evalæ¨¡å¼
            was_training = model.training
            model.eval()
            with torch.no_grad():
                result = model(patches)
            if was_training:
                model.train()
            return result
        else:
            # æ™®é€šæ¨¡å‹ï¼šä½¿ç”¨force_no_maskå‚æ•°
            return model(patches, force_no_mask=True)
    
    def extract_features(
        self,
        model: LightDenseNet3D,
        data: np.ndarray,
        batch_size: int = 16,
        device: Optional[str] = None
    ) -> np.ndarray:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æå–ç‰¹å¾
        
        Args:
            model: é¢„è®­ç»ƒçš„DenseNetæ¨¡å‹
            data: è¾“å…¥æ•°æ® [N, P, D, H, W]
            batch_size: æ‰¹æ¬¡å¤§å°
            device: è®¾å¤‡ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„è®¾å¤‡ï¼‰
            
        Returns:
            ç‰¹å¾æ•°ç»„ [N, P, feature_dim]
        """
        if device is None:
            device = self.config['device']
        
        device = torch.device(device)
        model = model.to(device)
        model.eval()
        
        all_features = []
        
        logger.info(f"Extracting features from {len(data)} samples...")
        
        with torch.no_grad():
            for i in range(len(data)):
                sample_patches = data[i]  # [P, D, H, W]
                
                # è½¬æ¢ä¸ºtensor
                patches_tensor = torch.FloatTensor(sample_patches).unsqueeze(1).to(device)
                
                # æ‰¹é‡å¤„ç†patch
                sample_features = []
                for j in range(0, len(patches_tensor), batch_size):
                    batch_patches = patches_tensor[j:j+batch_size]
                    features = model(batch_patches)
                    sample_features.append(features.cpu())
                
                # åˆå¹¶ç‰¹å¾
                sample_features = torch.cat(sample_features, dim=0).numpy()
                all_features.append(sample_features)
        
        features_array = np.stack(all_features, axis=0)
        logger.info(f"Feature extraction complete: {features_array.shape}")
        
        return features_array