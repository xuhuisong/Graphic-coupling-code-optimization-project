"""
DenseNet Pretraining Manager
DenseNeté¢„è®­ç»ƒè‡ªåŠ¨åŒ–ç®¡ç†å™¨

[ä¿®æ”¹ç‰ˆ]:
- ç§»é™¤äº†ä¸¤é˜¶æ®µè®­ç»ƒ (Stage 2 Masking)ã€‚
- æ›¿æ¢ä¸ºå•é˜¶æ®µè®­ç»ƒ + æ—©åœæ³• (Early Stopping)ã€‚
- è®­ç»ƒé›†ä½¿ç”¨ Z-Score + å¿«é€Ÿæ•°æ®å¢å¼ºã€‚
- éªŒè¯é›†å’Œç‰¹å¾æå–ä½¿ç”¨ Z-Scoreã€‚
"""

import os
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from typing import Dict, Any, Optional, Tuple
from pathlib import Path
import math
import pickle

from utils.checkpoint import CheckpointManager
from models.densenet import LightDenseNet3D, EndToEndDenseNet
# å¯¼å…¥æˆ‘ä»¬ä¿®æ”¹åçš„ PatchDataset
from data.dataset import PatchDataset, collate_fn, get_fold_splits

# [æ–°å¯¼å…¥] å¯¼å…¥ MONAI
try:
    from monai.transforms import Compose, RandFlip, RandAffine, RandGaussianNoise
except ImportError:
    logging.error("MONAI not found. Please install: pip install monai")
    exit()

logger = logging.getLogger(__name__)

# [æ–°å¢] æ•°æ®å¢å¼ºå®šä¹‰
def get_train_transform():
    """
    å®šä¹‰ä¸€ä¸ªå¿«é€Ÿã€ä¸€è‡´çš„æ•°æ®å¢å¼ºç®¡é“
    å®ƒå°† (P, D, H, W) è§†ä¸º (C, H, W, D) å¹¶å¯¹ H,W,D ç©ºé—´è¿›è¡Œå˜æ¢ã€‚
    """
    return Compose([
        RandFlip(spatial_axis=0, prob=0.5), 
        RandAffine(
            prob=0.5,
            rotate_range=(math.pi/32, math.pi/32, math.pi/32),
            translate_range=(3, 3, 3),
            scale_range=(0.05, 0.05, 0.05),
            padding_mode='border'
        ),
        RandGaussianNoise(prob=0.5, std=0.05)
    ])


class DenseNetManager:
    """
    DenseNeté¢„è®­ç»ƒç®¡ç†å™¨
    (åŠŸèƒ½æè¿°ä¿æŒä¸å˜)
    """
    
    def __init__(
        self,
        data_dir: str,
        checkpoint_manager: CheckpointManager,
        config: Optional[Dict[str, Any]] = None
    ):
        self.data_dir = data_dir
        self.checkpoint_manager = checkpoint_manager
        
        # [ä¿®æ”¹ç‚¹ 1] ç®€åŒ–é…ç½®
        self.config = {
            'growth_rate': 8,
            'num_init_features': 24,
            'num_epochs': 70,      # æ€» Epoch ä¸Šé™
            'patience': 40,         # æ—©åœçš„è€å¿ƒå€¼
            'batch_size': 24,
            'learning_rate': 0.0005,  # å•ä¸€å­¦ä¹ ç‡
            'weight_decay': 1e-4,     # å•ä¸€æƒé‡è¡°å‡
            'num_workers': 4,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }
                # æ›´æ–°ç”¨æˆ·æä¾›çš„é…ç½®
        if config:
            self.config.update(config)
        logger.info(f"DenseNetManager initialized with data_dir: {data_dir}")
        logger.info(f"Training config: {self.config}")
    
    def get_pretrained_model(
        self,
        fold: int,
        split_seed: int,
        force_retrain: bool = False
    ) -> LightDenseNet3D:
        """
        è·å–é¢„è®­ç»ƒçš„DenseNetæ¨¡å‹
        """
        # [ä¿®æ”¹ç‚¹ 2] æ›´æ–°ç¼“å­˜æ ‡è¯†ç¬¦
        config_params = {
            'growth_rate': self.config['growth_rate'],
            'num_init_features': self.config['num_init_features'],
            'num_epochs': self.config['num_epochs'],
            'patience': self.config['patience'],
            'learning_rate': self.config['learning_rate']
            # ç§»é™¤äº† stage2/mask å‚æ•°
        }
        
        identifier = self.checkpoint_manager.build_identifier(
            'densenet',
            config_params,
            {'fold': fold, 'seed': split_seed}
        )
        
        cache_exists = self.checkpoint_manager.check_exists('densenet', identifier)
        
        if cache_exists and not force_retrain:
            logger.info(f"[Fold {fold}] Loading cached pretrained DenseNet: {identifier}")
            return self._load_pretrained_model(identifier)
        else:
            if force_retrain:
                logger.info(f"[Fold {fold}] Force retraining DenseNet...")
            else:
                logger.info(f"[Fold {fold}] No cache found, starting training...")
            
            return self._train_and_save(fold, split_seed, identifier, config_params)
    
    def _load_pretrained_model(self, identifier: str) -> LightDenseNet3D:
        """ä»ç¼“å­˜åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        # (æ­¤å‡½æ•°é€»è¾‘ä¿æŒä¸å˜)
        checkpoint = self.checkpoint_manager.load(
            'densenet',
            identifier,
            map_location='cpu'
        )
        
        model = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"Loaded from epoch {checkpoint.get('epoch', 'unknown')}, "
                       f"val_acc: {checkpoint.get('val_acc', 'unknown'):.4f}")
        else:
            model.load_state_dict(checkpoint)
        
        logger.info(f"Successfully loaded pretrained DenseNet")
        return model
    
    def _train_and_save(
        self,
        fold: int,
        split_seed: int,
        identifier: str,
        config_params: Dict[str, Any]
    ) -> LightDenseNet3D:
        """è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹"""
        
        logger.info(f"\n{'='*70}")
        logger.info(f"Starting DenseNet Pretraining - Fold {fold}")
        logger.info(f"{'='*70}\n")
        
        # 1. å‡†å¤‡æ•°æ®
        train_loader, val_loader = self._prepare_dataloaders(fold, split_seed)
        
        # 2. åˆ›å»ºæ¨¡å‹
        device = torch.device(self.config['device'])
        
        feature_extractor = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        
        try:
            num_patches = val_loader.dataset.dataset.get_num_patches()
        except Exception:
            temp_dataset = PatchDataset(self.data_dir, transform=None)
            num_patches = temp_dataset.get_num_patches()
            del temp_dataset

        
        model = EndToEndDenseNet(feature_extractor, num_patches).to(device)
        
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            logger.info(f"Using {torch.cuda.device_count()} GPUs with DataParallel")
            model = nn.DataParallel(model)
        
        # 3. [ä¿®æ”¹ç‚¹ 3] æ‰§è¡Œå•é˜¶æ®µè®­ç»ƒ
        best_model_path = self._train_model_with_early_stopping(
            model,
            train_loader,
            val_loader,
            device,
            fold,
            num_epochs=self.config['num_epochs'],
            patience=self.config['patience']
        )
        
        # 4. ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°ç¼“å­˜
        logger.info(f"Saving best model to cache...")
        best_checkpoint = torch.load(best_model_path, map_location='cpu')
        
        # (ä¿æŒä¸å˜ï¼šä¿å­˜ extractor çš„ state_dict)
        feature_extractor_state = best_checkpoint['model_state_dict']
        
        save_data = {
            'model_state_dict': feature_extractor_state,
            'epoch': best_checkpoint['epoch'],
            'val_acc': best_checkpoint['val_acc'],
            'val_loss': best_checkpoint['val_loss']
        }
        
        self.checkpoint_manager.save(
            'densenet',
            identifier,
            save_data,
            config_params
        )
        
        if os.path.exists(best_model_path):
            os.remove(best_model_path)
        
        logger.info(f"\n{'='*70}")
        logger.info(f"DenseNet Pretraining Completed - Fold {fold}")
        logger.info(f"Best Val Acc: {best_checkpoint['val_acc']:.4f}")
        logger.info(f"{'='*70}\n")
        
        clean_feature_extractor = LightDenseNet3D(
            growth_rate=self.config['growth_rate'],
            num_init_features=self.config['num_init_features']
        )
        clean_feature_extractor.load_state_dict(feature_extractor_state)
        
        return clean_feature_extractor
    
    def _prepare_dataloaders(
        self,
        fold: int,
        split_seed: int
    ) -> Tuple[DataLoader, DataLoader]:
        """
        å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨
        [ä¿®æ”¹ç‚¹ 4]ï¼šä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ›å»ºä¸åŒçš„ Dataset å®ä¾‹
        """
        
        # 1. ä¸ºè®­ç»ƒé›†åˆ›å»ºå¸¦å¢å¼ºçš„ Dataset
        train_transform = get_train_transform()
        train_dataset = PatchDataset(self.data_dir, transform=train_transform)
        
        # 2. ä¸ºéªŒè¯é›†åˆ›å»ºä¸å¸¦å¢å¼ºçš„ Dataset
        eval_dataset = PatchDataset(self.data_dir, transform=None)
        
        # 3. è·å–æ•°æ®åˆ†å‰²ç´¢å¼•
        train_indices, val_indices, _ = get_fold_splits(
            self.data_dir, fold, split_seed
        )
        
        logger.info(f"Data split - Train: {len(train_indices)}, Val: {len(val_indices)}")
        
        # 4. ä»å„è‡ªçš„æ•°æ®é›†ä¸­åˆ›å»ºå­é›†
        train_subset = Subset(train_dataset, train_indices)
        val_subset = Subset(eval_dataset, val_indices)
        
        # 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_subset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_subset,
            batch_size=self.config['batch_size'],
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=self.config['num_workers'],
            pin_memory=True
        )
        
        return train_loader, val_loader
    
    # [ä¿®æ”¹ç‚¹ 5] æ–°çš„è®­ç»ƒå‡½æ•° (æ›¿æ¢ _two_stage_training)
    def _train_model_with_early_stopping(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        device: torch.device,
        fold: int,
        num_epochs: int,
        patience: int
    ) -> str:
        """
        å•é˜¶æ®µè®­ç»ƒç­–ç•¥ + æ—©åœæ³•
        
        Returns:
            æœ€ä½³æ¨¡å‹çš„ä¸´æ—¶ä¿å­˜è·¯å¾„
        """
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0.0
        best_val_loss = float('inf')
        best_epoch = -1
        patience_counter = 0
        
        temp_save_path = f'./temp_densenet_fold{fold}_best.pth'
        
        logger.info(f"Starting Single-Stage Training (Epochs: {num_epochs}, Patience: {patience})")
        
        optimizer = optim.Adam(
            model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_epochs, eta_min=1e-6
        )
        
        actual_model = model.module if isinstance(model, nn.DataParallel) else model
        actual_model.mask_ratio = 0.0 # ç¡®ä¿ mask å§‹ç»ˆå…³é—­
        
        for epoch in range(num_epochs):
            
            # è®­ç»ƒ
            train_loss, train_acc = self._train_one_epoch(
                model, train_loader, criterion, optimizer, device, mask_ratio=0.0
            )
            
            # éªŒè¯
            val_loss, val_acc = self._validate(
                model, val_loader, criterion, device
            )
            
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # æ—©åœé€»è¾‘
            if val_acc > best_val_acc or (val_acc == best_val_acc and val_loss < best_val_loss):
                best_val_acc = val_acc
                best_val_loss = val_loss
                best_epoch = epoch
                patience_counter = 0 # é‡ç½®è€å¿ƒ
                
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': actual_model.feature_extractor.state_dict(),
                    'val_acc': val_acc,
                    'val_loss': val_loss
                }, temp_save_path)
                
                logger.info(f"ğŸ’ [New Best] Epoch {epoch+1}: Val Acc={val_acc:.4f}, Val Loss={val_loss:.4f}")
            else:
                patience_counter += 1
                logger.info(f"  (No improvement, patience: {patience_counter}/{patience})")

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            logger.info(
                f"Epoch [{epoch+1}/{num_epochs}] | "
                f"LR: {current_lr:.1e} | "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}"
            )
            
            if patience_counter >= patience:
                logger.info(f"\nğŸ”¥ [Triggering Early Stopping] Validation accuracy did not improve for {patience} epochs.")
                break # åœæ­¢è®­ç»ƒ
        
        logger.info(f"\nBest model from Epoch {best_epoch+1}: "
                   f"Val Acc={best_val_acc:.4f}, Val Loss={best_val_loss:.4f}")
        
        return temp_save_path
    
    # [ä¿®æ”¹ç‚¹ 6] ç®€åŒ– _train_one_epoch
    def _train_one_epoch(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: torch.device,
        mask_ratio: float # (æ­¤å‚æ•°ä¿ç•™ä»¥åŒ¹é…å‡½æ•°ç­¾åï¼Œä½†å§‹ç»ˆä¸º 0)
    ) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch (å·²ç§»é™¤ mask é€»è¾‘)"""
        model.train()
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        for patches, _, labels in train_loader:
            patches = patches.to(device)
            labels = labels.to(device)
            
            # å‰å‘ä¼ æ’­
            outputs = model(patches) # force_no_mask é»˜è®¤ä¸º False
            loss = criterion(outputs, labels)
            
            # (ç§»é™¤äº† mask å’Œä¸€è‡´æ€§æŸå¤±)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
    
    # [ä¿®æ”¹ç‚¹ 7] ç®€åŒ– _validate
    def _validate(
        self,
        model: nn.Module,
        val_loader: DataLoader,
        criterion: nn.Module,
        device: torch.device
    ) -> Tuple[float, float]:
        """éªŒè¯æ¨¡å‹ (å·²ç§»é™¤ _forward_no_mask)"""
        model.eval()
        
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for patches, _, labels in val_loader:
                patches = patches.to(device)
                labels = labels.to(device)
                
                # model.eval() ä¼šè‡ªåŠ¨å¤„ç† (mask_ratio=0)
                outputs = model(patches) 
                loss = criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy

    def extract_features(
            self,
            model: LightDenseNet3D,
            data: np.ndarray,
            batch_size: int = 16,
            device: Optional[str] = None
        ) -> np.ndarray:
            """
            ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æå–ç‰¹å¾
            [ä¿®æ”¹]ï¼šå¢åŠ äº† Z-Score å½’ä¸€åŒ–ä»¥åŒ¹é…è®­ç»ƒ
            """
            if device is None:
                device = self.config['device']

            device = torch.device(device)
            model = model.to(device)
            model.eval()

            all_features = []

            logger.info(f"Extracting features from {len(data)} samples...")

            with torch.no_grad():
                for i in range(len(data)):
                    sample_patches = data[i]  # [P, D, H, W]

                    # [æ ¸å¿ƒä¿®æ”¹] è½¬æ¢ä¸º tensor å¹¶åº”ç”¨ Z-Score
                    patches_tensor = torch.from_numpy(sample_patches).float() # (P, D, H, W)

                    p_mean = patches_tensor.mean()
                    p_std = patches_tensor.std()
                    patches_tensor = (patches_tensor - p_mean) / (p_std + 1e-6)

                    # æ·»åŠ é€šé“ç»´åº¦å¹¶ç§»åˆ° GPU
                    patches_tensor = patches_tensor.unsqueeze(1).to(device) # (P, 1, D, H, W)

                    sample_features = []
                    for j in range(0, len(patches_tensor), batch_size):
                        batch_patches = patches_tensor[j:j+batch_size]
                        features = model(batch_patches)
                        sample_features.append(features.cpu())

                    sample_features = torch.cat(sample_features, dim=0).numpy()
                    all_features.append(sample_features)

            features_array = np.stack(all_features, axis=0)
            logger.info(f"Feature extraction complete: {features_array.shape}")

            return features_array