"""
å› æœå›¾ç¥ç»ç½‘ç»œæ¨¡å‹
(åŠ¨æ€è¾¹æ„å»º + æ›¿æ¢å¼èåˆç‰ˆæœ¬)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Tuple, Optional


class GCN(nn.Module):
    """å›¾å·ç§¯å±‚ (AXW)"""
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True):
        super(GCN, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.W = nn.Linear(in_features, out_features, bias=bias)
        self.reset_parameters()
    
    def reset_parameters(self):
        self.W.reset_parameters()
    
    def forward(self, X: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:
        """
        Args:
            X: èŠ‚ç‚¹ç‰¹å¾ [B, P, d]
            adj: é‚»æ¥çŸ©é˜µ [B, P, P]
        Returns:
            è¾“å‡ºç‰¹å¾ [B, P, d']
        """
        XW = self.W(X)
        AXW = torch.bmm(adj, XW)
        return AXW


class GCNBlock(nn.Module):
    """
    ã€é‡æ„ã€‘
    GCNæ¨¡å—ï¼Œä»…è´Ÿè´£å­å›¾å†…éƒ¨çš„ç‰¹å¾å­¦ä¹ 
    (ç‰ˆæœ¬: ä»…å«Dropoutï¼Œæ— BNæˆ–ReLU)
    """
    
    def __init__(self, in_dim: int, hidden: list, dropout_p: float = 0.2):
        """
        Args:
            in_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden: éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼Œä¾‹å¦‚ [hidden1_dim, output_dim]
            dropout_p: Dropout æ¦‚ç‡
        """
        super(GCNBlock, self).__init__()
        self.hidden = hidden
        
        # ä¸¤å±‚GCN
        self.conv1 = GCN(in_features=in_dim, out_features=self.hidden[0])
        self.dropout1 = nn.Dropout(dropout_p)
        
        self.conv2 = GCN(in_features=self.hidden[0], out_features=self.hidden[-1])
        self.dropout2 = nn.Dropout(dropout_p)
    
    def forward(self, x: torch.Tensor, edge1: torch.Tensor) -> torch.Tensor:
        """
        åªå¤„ç†å­å›¾å†…éƒ¨çš„å·ç§¯
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [B, P, d]
            edge1: å­å›¾å†…éƒ¨è¾¹ [B, P, P]
        """
        # ========== ç¬¬ä¸€å±‚ ==========
        edge1_norm = self._normalize_adj_batch(edge1)
        x = self.conv1(x, edge1_norm)
        # ç§»é™¤äº† F.relu å’Œ bn
        x = self.dropout1(x)
        
        # ========== ç¬¬äºŒå±‚ ==========
        # æ³¨æ„ï¼šç¬¬äºŒå±‚å·ç§¯ä»ç„¶ä½¿ç”¨ç¬¬ä¸€å±‚å½’ä¸€åŒ–åçš„é‚»æ¥çŸ©é˜µ
        x = self.conv2(x, edge1_norm) 
        # ç§»é™¤äº† F.relu å’Œ bn
        x = self.dropout2(x)
        
        return x
    
    @staticmethod
    def _normalize_adj_batch(adj: torch.Tensor) -> torch.Tensor:
        """
        å¯¹ç§°å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ (A + I)
        (è¿™ä¸ªå‡½æ•°ä¿æŒä¸å˜)
        """
        batch_size, num_nodes = adj.shape[0], adj.shape[1]
        adj_clone = adj.clone()

        # 1. å¼ºåˆ¶æ¸…ç©ºå¯¹è§’çº¿ï¼ˆç§»é™¤ä»»ä½•å·²æœ‰çš„è‡ªç¯ï¼‰
        diag_indices = torch.arange(num_nodes, device=adj.device)
        adj_clone[:, diag_indices, diag_indices] = 0
        
        # 2. æ·»åŠ è‡ªç¯ï¼ˆç»Ÿä¸€æƒé‡ä¸º1ï¼‰
        identity = torch.eye(num_nodes, device=adj.device).unsqueeze(0)
        adj_with_self_loops = adj_clone + identity

        # 3. è®¡ç®—åº¦
        degree = adj_with_self_loops.sum(dim=2) # [B, P]

        # 4. D^{-1/2}
        degree_inv_sqrt = torch.pow(degree, -0.5)
        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.0 # å¤„ç†å­¤ç«‹èŠ‚ç‚¹

        # 5. å¯¹ç§°å½’ä¸€åŒ–: D^{-1/2} * A * D^{-1/2}
        adj_normalized = (degree_inv_sqrt.unsqueeze(2) * adj_with_self_loops * degree_inv_sqrt.unsqueeze(1))

        return adj_normalized
    
    @staticmethod
    def _normalize_adj_batch(adj: torch.Tensor) -> torch.Tensor:
        """
        å¯¹ç§°å½’ä¸€åŒ–é‚»æ¥çŸ©é˜µ (A + I)
        """
        batch_size, num_nodes = adj.shape[0], adj.shape[1]
        adj_clone = adj.clone()

        # 1. å¼ºåˆ¶æ¸…ç©ºå¯¹è§’çº¿ï¼ˆç§»é™¤ä»»ä½•å·²æœ‰çš„è‡ªç¯ï¼‰
        diag_indices = torch.arange(num_nodes, device=adj.device)
        adj_clone[:, diag_indices, diag_indices] = 0
        
        # 2. æ·»åŠ è‡ªç¯ï¼ˆç»Ÿä¸€æƒé‡ä¸º1ï¼‰
        identity = torch.eye(num_nodes, device=adj.device).unsqueeze(0)
        adj_with_self_loops = adj_clone + identity

        # 3. è®¡ç®—åº¦
        degree = adj_with_self_loops.sum(dim=2) # [B, P]

        # 4. D^{-1/2}
        degree_inv_sqrt = torch.pow(degree, -0.5)
        degree_inv_sqrt[torch.isinf(degree_inv_sqrt)] = 0.0 # å¤„ç†å­¤ç«‹èŠ‚ç‚¹

        # 5. å¯¹ç§°å½’ä¸€åŒ–: D^{-1/2} * A * D^{-1/2}
        adj_normalized = (degree_inv_sqrt.unsqueeze(2) * adj_with_self_loops * degree_inv_sqrt.unsqueeze(1))

        return adj_normalized


def readout(x: torch.Tensor) -> torch.Tensor:
    """
    å›¾è¯»å‡ºå‡½æ•° [B, P, d] -> [B, P*d]
    """
    batch_size = x.shape[0]
    return x.reshape(batch_size, -1)


class CausalNet(nn.Module):
    """
    ã€é‡æ„ã€‘
    å› æœå›¾ç¥ç»ç½‘ç»œ
    ä½¿ç”¨ GCNBlock è¿›è¡Œå†…éƒ¨ç‰¹å¾æå–
    ä½¿ç”¨ _perform_fusion_replacement (æ›¿æ¢å¼èåˆ) è¿›è¡Œå› æœæµ‹è¯•
    """
    
    def __init__(
        self,
        num_class: int,
        feature_dim: int,
        hidden1: list, # (æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç­¾å)
        hidden2: list,
        num_patches: int,
        kernels: Optional[list] = None, # (æœªä½¿ç”¨ï¼Œä½†ä¿ç•™ç­¾å)
        num_neg_samples: int = 4
    ):
        super(CausalNet, self).__init__()
        
        self.num_neg_samples_default = num_neg_samples
        self.num_class = num_class
        self.feature_dim = feature_dim
        self.num_patches = num_patches # å¯¹åº” P
        self.hidden2 = hidden2
        
        # GCN æ¨¡å—ï¼ˆç”¨äºå­å›¾å†…éƒ¨ç‰¹å¾æå–ï¼‰
        self.gcn_block = GCNBlock(
            in_dim=feature_dim,
            hidden=hidden2
        )
        
        # å› æœMLPï¼ˆæ‰€æœ‰å› æœè·¯å¾„å…±ç”¨ï¼‰
        causal_feature_size = hidden2[-1] * num_patches
        self.mlp_causal = nn.Sequential(
            nn.Linear(causal_feature_size, 128),
            nn.ReLU(True),
            nn.Dropout(0.2),
            nn.Linear(128, num_class))
    
    
    def compute_dynamic_edges(
        self,
        x: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        is_large_graph: bool = False
    ) -> torch.Tensor:
        """
        åŸºäºè¾“å…¥ç‰¹å¾åŠ¨æ€è®¡ç®—ä¸ªæ€§åŒ–è¾¹çŸ©é˜µ (æœªæ”¹åŠ¨)
        """
        B, total_P, d = x.shape
        P = edge_prior_mask.shape[0]
        
        if is_large_graph:
            num_subgraphs = total_P // P
            all_edges = []
            for i in range(num_subgraphs):
                start = i * P
                end = (i + 1) * P
                sub_x = x[:, start:end, :]
                
                sub_x_norm = F.normalize(sub_x, p=2, dim=2)
                sub_similarity = torch.bmm(sub_x_norm, sub_x_norm.transpose(1, 2))
                sub_similarity = (sub_similarity + 1) / 2
                
                mask_expanded = edge_prior_mask.unsqueeze(0).expand(B, -1, -1)
                sub_edges = sub_similarity * mask_expanded
                
                all_edges.append(sub_edges)
            
            edges = self._assemble_block_diagonal(all_edges)
            
        else:
            x_norm = F.normalize(x, p=2, dim=2)
            similarity = torch.bmm(x_norm, x_norm.transpose(1, 2))
            similarity = (similarity + 1) / 2
            
            mask_expanded = edge_prior_mask.unsqueeze(0).expand(B, -1, -1)
            edges = similarity * mask_expanded
        
        # ç§»é™¤è‡ªç¯ï¼ˆå¯¹è§’çº¿ç½®0ï¼‰
        num_nodes = edges.shape[1]
        identity = torch.eye(num_nodes, device=edges.device).unsqueeze(0).expand(B, -1, -1)
        edges = edges * (1 - identity)
        
        return edges
    
    def _assemble_block_diagonal(self, block_list: list) -> torch.Tensor:
        """
        ç»„è£…å—å¯¹è§’çŸ©é˜µ (æœªæ”¹åŠ¨)
        """
        B = block_list[0].shape[0]
        P = block_list[0].shape[1]
        num_blocks = len(block_list)
        total_P = num_blocks * P
        
        device = block_list[0].device
        large_edges = torch.zeros(B, total_P, total_P, device=device)
        
        for idx, block in enumerate(block_list):
            start = idx * P
            end = (idx + 1) * P
            large_edges[:, start:end, start:end] = block
        
        return large_edges
    
    def prediction_whole(
        self,
        x_new: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        å…¨å›¾é¢„æµ‹ - ä½¿ç”¨åŠ¨æ€è®¡ç®—çš„è¾¹çŸ©é˜µ (å·²æ›´æ–°)
        """
        edge = self.compute_dynamic_edges(x_new, edge_prior_mask, is_large_graph)
        
        # ã€æ›´æ–°ã€‘ä½¿ç”¨ GCNBlock
        xs = self.gcn_block(x_new, edge)
        
        graph = readout(xs)
        return self.mlp_causal(graph)
    
    def prediction_intrinsic_path(
        self,
        x_new: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        å› æœä¸å˜æ€§é¢„æµ‹ï¼ˆå†…åœ¨è·¯å¾„ï¼‰(å·²æ›´æ–°)
        """
        node_mask, edge_mask = masks
        
        edge = self.compute_dynamic_edges(x_new, edge_prior_mask, is_large_graph)
        
        if is_large_graph:
            P = self.num_patches
            x_orig = x_new[:, :P, :].clone()
            edge_orig = edge[:, :P, :P].clone()
            
            x_masked = x_orig * node_mask.unsqueeze(0).unsqueeze(-1)
            inter_node_adj = edge_orig * edge_mask.unsqueeze(0)
            
            # ã€æ›´æ–°ã€‘ä½¿ç”¨ GCNBlock
            xs = self.gcn_block(x_masked, inter_node_adj)
        else:
            x_masked = x_new * node_mask.unsqueeze(0).unsqueeze(-1)
            inter_node_adj = edge * edge_mask.unsqueeze(0)
            
            # ã€æ›´æ–°ã€‘ä½¿ç”¨ GCNBlock
            xs = self.gcn_block(x_masked, inter_node_adj)
        
        graph = readout(xs)
        return self.mlp_causal(graph)
    
    def prediction_spurious_path(
        self,
        x_new: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        å› æœå˜å¼‚æ€§é¢„æµ‹ï¼ˆè™šå‡è·¯å¾„ï¼‰(å·²æ›´æ–°)
        """
        node_mask, edge_mask = masks
        
        edge = self.compute_dynamic_edges(x_new, edge_prior_mask, is_large_graph)
        
        causal_mask = node_mask.unsqueeze(0).unsqueeze(-1)
        x_perturbed = x_new * (1 - causal_mask)
        
        causal_edge_mask = edge_mask.unsqueeze(0)
        edge_perturbed = edge * (1 - causal_edge_mask)
        
        # ã€æ›´æ–°ã€‘ä½¿ç”¨ GCNBlock
        xs = self.gcn_block(x_perturbed, edge_perturbed)
        
        graph = readout(xs)
        return self.mlp_causal(graph)

    # -----------------------------------------------------------------
    # ã€å…¨æ–°ã€‘æ›¿æ¢å¼èåˆ (Replacement Fusion) é€»è¾‘
    # -----------------------------------------------------------------
    def _perform_fusion_replacement(
        self,
        x: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        fusion_type: str, # "intrinsic" æˆ– "spurious"
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        ã€æ–°ã€‘æ‰§è¡Œæ›¿æ¢å¼èåˆçš„ç§æœ‰è¾…åŠ©å‡½æ•°
        
        æµç¨‹:
        1. å¯¹å¤§å›¾ä¸­çš„æ‰€æœ‰å­å›¾ï¼ˆAnchor + Negativesï¼‰æ‰§è¡Œå†…éƒ¨GCNï¼ˆé˜¶æ®µä¸€ï¼‰ã€‚
        2. èšåˆæ‰€æœ‰è´Ÿæ ·æœ¬ï¼ˆNegativesï¼‰çš„ç‰¹å¾ï¼Œå¾—åˆ° "æ›¿æ¢æº" ç‰¹å¾ã€‚
        3. æ ¹æ® fusion_typeï¼Œæœ‰æ¡ä»¶åœ°ç”¨ "æ›¿æ¢æº" æ›¿æ¢ Anchor çš„ "å› æœ" æˆ– "è™šå‡" èŠ‚ç‚¹ã€‚
        4. å¯¹è¢«æ›¿æ¢åçš„ Anchor å›¾è¿›è¡Œé¢„æµ‹ã€‚
        """
        node_mask, edge_mask = masks

        if not is_large_graph:
            raise ValueError("æ›¿æ¢å¼èåˆ (Replacement Fusion) éœ€è¦å¤§å›¾æ¨¡å¼")

        B, large_P, d = x.shape
        P = self.num_patches

        if large_P % P != 0:
            raise ValueError(f"å¤§å›¾èŠ‚ç‚¹æ•° ({large_P}) ä¸æ˜¯ P ({P}) çš„æ•´æ•°å€")
        
        num_subgraphs = large_P // P
        num_neg_samples = num_subgraphs - 1
        
        if num_neg_samples == 0:
             raise ValueError("æ›¿æ¢å¼èåˆéœ€è¦è‡³å°‘ä¸€ä¸ªè´Ÿæ ·æœ¬")

        # âœ… ç¬¬1æ­¥ï¼šåŠ¨æ€è®¡ç®—æ¯ä¸ªå­å›¾çš„ "å› æœ" è¾¹
        full_dynamic_edges = self.compute_dynamic_edges(x, edge_prior_mask, is_large_graph=True)
        
        intrinsic_edge_mask_large = torch.zeros(large_P, large_P, device=x.device)
        for i in range(num_subgraphs):
            start = i * P
            end = (i + 1) * P
            intrinsic_edge_mask_large[start:end, start:end] = edge_mask

        internal_edges = full_dynamic_edges * intrinsic_edge_mask_large.unsqueeze(0)

        # âœ… ç¬¬2æ­¥ï¼šæ‰§è¡Œé˜¶æ®µä¸€ï¼ˆæ‰€æœ‰å­å›¾çš„å†…éƒ¨å·ç§¯ï¼‰
        # ä½¿ç”¨ gcn_blockï¼Œå®ƒåªæ‰§è¡Œå†…éƒ¨å·ç§¯
        xs_stage1 = self.gcn_block(x, internal_edges)
        # xs_stage1 å½¢çŠ¶ [B, (N+1)*P, d]

        # âœ… ç¬¬3æ­¥ï¼šå‡†å¤‡æ›¿æ¢æ•°æ®
        
        # 1. åˆ†ç¦» Anchor å’Œ Negatives
        x_anchor_s1 = xs_stage1[:, :P, :]  # Anchorç‰¹å¾ [B, P, d]
        x_negs_s1 = xs_stage1[:, P:, :]  # æ‰€æœ‰è´Ÿæ ·æœ¬ç‰¹å¾ [B, N*P, d]
        
        # 2. é‡å¡‘å¹¶èšåˆè´Ÿæ ·æœ¬
        x_negs_s1_reshaped = x_negs_s1.reshape(B, num_neg_samples, P, d) # [B, N, P, d]
        
        # 3. è®¡ç®—ç”¨äºæ›¿æ¢çš„"èšåˆä¿¡æ¯" (å–å¹³å‡)
        x_replacement_info = torch.mean(x_negs_s1_reshaped, dim=1) # [B, P, d]

        # âœ… ç¬¬4æ­¥ï¼šæ‰§è¡Œâ€œæ¡ä»¶æ›¿æ¢â€
        
        # å‡†å¤‡æ©ç 
        mask_intrinsic = node_mask.reshape(1, P, 1).to(x.device) # å› æœèŠ‚ç‚¹ [1, P, 1]
        mask_spurious = (1 - mask_intrinsic)                     # è™šå‡èŠ‚ç‚¹ [1, P, 1]

        if fusion_type == "intrinsic":
            # å†…åœ¨èåˆ: Anchorçš„å› æœéƒ¨åˆ†è¢«æ›¿æ¢ï¼Œè™šå‡éƒ¨åˆ†ä¿ç•™
            x_anchor_new = (x_replacement_info * mask_intrinsic) + \
                           (x_anchor_s1 * mask_spurious)
        
        elif fusion_type == "spurious":
            # è™šå‡èåˆ: Anchorçš„è™šå‡éƒ¨åˆ†è¢«æ›¿æ¢ï¼Œå› æœéƒ¨åˆ†ä¿ç•™
            x_anchor_new = (x_anchor_s1 * mask_intrinsic) + \
                           (x_replacement_info * mask_spurious)
        else:
            raise ValueError(f"æœªçŸ¥çš„ fusion_type: {fusion_type}")
        
        # âœ… ç¬¬5æ­¥ï¼šä½¿ç”¨è¢«æ›¿æ¢åçš„Anchorå›¾è¿›è¡Œé¢„æµ‹
        graph = readout(x_anchor_new) # x_anchor_new æ˜¯ [B, P, d]
        logits = self.mlp_causal(graph)
        
        return logits
    '''
    def _perform_fusion_replacement(
        self,
        x: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        fusion_type: str, # "intrinsic" æˆ– "spurious"
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        ã€æ–°ã€‘æ‰§è¡Œæ›¿æ¢å¼èåˆçš„ç§æœ‰è¾…åŠ©å‡½æ•°

        æµç¨‹:
        1. å¯¹å¤§å›¾ä¸­çš„æ‰€æœ‰å­å›¾ï¼ˆAnchor + Negativesï¼‰æ‰§è¡Œå†…éƒ¨GCNï¼ˆé˜¶æ®µä¸€ï¼‰ã€‚
        2. èšåˆæ‰€æœ‰è´Ÿæ ·æœ¬ï¼ˆNegativesï¼‰çš„ç‰¹å¾ï¼Œå¾—åˆ° "æ›¿æ¢æº" ç‰¹å¾ã€‚
        3. æ ¹æ® fusion_typeï¼Œæœ‰æ¡ä»¶åœ°ç”¨ "æ›¿æ¢æº" æ›¿æ¢ Anchor çš„ "å› æœ" æˆ– "è™šå‡" èŠ‚ç‚¹ã€‚
        4. å¯¹è¢«æ›¿æ¢åçš„ Anchor å›¾è¿›è¡Œé¢„æµ‹ã€‚
        """
        import logging
        logger = logging.getLogger(__name__)

        node_mask, edge_mask = masks

        if not is_large_graph:
            raise ValueError("æ›¿æ¢å¼èåˆ (Replacement Fusion) éœ€è¦å¤§å›¾æ¨¡å¼")

        B, large_P, d = x.shape
        P = self.num_patches

        if large_P % P != 0:
            raise ValueError(f"å¤§å›¾èŠ‚ç‚¹æ•° ({large_P}) ä¸æ˜¯ P ({P}) çš„æ•´æ•°å€")

        num_subgraphs = large_P // P
        num_neg_samples = num_subgraphs - 1

        if num_neg_samples == 0:
             raise ValueError("æ›¿æ¢å¼èåˆéœ€è¦è‡³å°‘ä¸€ä¸ªè´Ÿæ ·æœ¬")

        # ========================================================================
        # ğŸ” è°ƒè¯•ç‚¹1: æ£€æŸ¥è¾“å…¥ç‰¹å¾
        # ========================================================================
        logger.info(f"")
        logger.info(f"{'='*80}")
        logger.info(f"DEBUG [Fusion {fusion_type}]: å¼€å§‹æ›¿æ¢å¼èåˆ")
        logger.info(f"{'='*80}")
        logger.info(f"DEBUG [Fusion {fusion_type}]: è¾“å…¥ç‰¹å¾ x ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  â”œâ”€ Shape: {x.shape}")
        logger.info(f"  â”œâ”€ Mean: {x.mean().item():.6f}, Std: {x.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {x.min().item():.6f}, Max: {x.max().item():.6f}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ NaN: {torch.isnan(x).any().item()}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ Inf: {torch.isinf(x).any().item()}")

        # ========================================================================
        # ğŸ” è°ƒè¯•ç‚¹: æ£€æŸ¥ edge_mask çš„å€¼åˆ†å¸ƒ
        # ========================================================================
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: edge_mask è¯¦ç»†ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {edge_mask.shape}")
        logger.info(f"  â”œâ”€ Dtype: {edge_mask.dtype}")
        logger.info(f"  â”œâ”€ Min: {edge_mask.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {edge_mask.max().item():.6f}")
        logger.info(f"  â”œâ”€ Mean: {edge_mask.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {edge_mask.std().item():.6f}")

        # ç»Ÿè®¡ä¸åŒé˜ˆå€¼ä¸‹çš„æ•°é‡
        logger.info(f"  â”œâ”€ å€¼ == 0 çš„æ•°é‡: {(edge_mask == 0).sum().item()}")
        logger.info(f"  â”œâ”€ å€¼ == 1 çš„æ•°é‡: {(edge_mask == 1).sum().item()}")
        logger.info(f"  â”œâ”€ å€¼ > 0 çš„æ•°é‡: {(edge_mask > 0).sum().item()}")
        logger.info(f"  â”œâ”€ å€¼ > 0.5 çš„æ•°é‡: {(edge_mask > 0.5).sum().item()}")
        logger.info(f"  â”œâ”€ å€¼ > 0.9 çš„æ•°é‡: {(edge_mask > 0.9).sum().item()}")

        # æ˜¾ç¤ºä¸€äº›å®é™…çš„å”¯ä¸€å€¼
        unique_vals = torch.unique(edge_mask)
        if len(unique_vals) <= 20:
            logger.info(f"  â”œâ”€ æ‰€æœ‰å”¯ä¸€å€¼: {unique_vals.tolist()}")
        else:
            logger.info(f"  â”œâ”€ å‰20ä¸ªå”¯ä¸€å€¼: {unique_vals[:20].tolist()}")
            logger.info(f"  â”œâ”€ å”¯ä¸€å€¼æ€»æ•°: {len(unique_vals)}")

        # ========================================================================
        # âœ… ç¬¬1æ­¥ï¼šåŠ¨æ€è®¡ç®—æ¯ä¸ªå­å›¾çš„è¾¹
        # ========================================================================
        full_dynamic_edges = self.compute_dynamic_edges(x, edge_prior_mask, is_large_graph=True)

        # ğŸ” è°ƒè¯•ç‚¹2: æ£€æŸ¥åŠ¨æ€è¾¹
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: åŠ¨æ€è¾¹ full_dynamic_edges ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {full_dynamic_edges.shape}")
        logger.info(f"  â”œâ”€ Mean: {full_dynamic_edges.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {full_dynamic_edges.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {full_dynamic_edges.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {full_dynamic_edges.max().item():.6f}")
        logger.info(f"  â”œâ”€ éé›¶è¾¹æ•°: {(full_dynamic_edges > 0).sum().item()}")
        logger.info(f"  â”œâ”€ æ€»è¾¹æ•°: {full_dynamic_edges.numel()}")

        # æ„å»ºå†…éƒ¨è¾¹æ©ç ï¼ˆåªä¿ç•™å› æœè¾¹ï¼‰
        intrinsic_edge_mask_large = torch.zeros(large_P, large_P, device=x.device)
        for i in range(num_subgraphs):
            start = i * P
            end = (i + 1) * P
            intrinsic_edge_mask_large[start:end, start:end] = edge_mask

        internal_edges = full_dynamic_edges * intrinsic_edge_mask_large.unsqueeze(0)

        # ğŸ” è°ƒè¯•ç‚¹3: æ£€æŸ¥å†…éƒ¨è¾¹ï¼ˆåº”ç”¨æ©ç åï¼‰
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: å†…éƒ¨è¾¹ internal_edges ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {internal_edges.shape}")
        logger.info(f"  â”œâ”€ Mean: {internal_edges.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {internal_edges.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {internal_edges.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {internal_edges.max().item():.6f}")
        logger.info(f"  â”œâ”€ éé›¶è¾¹æ•° (>0): {(internal_edges > 0).sum().item()}")
        logger.info(f"  â”œâ”€ éé›¶è¾¹æ•° (>0.01): {(internal_edges > 0.01).sum().item()}")
        logger.info(f"  â”œâ”€ éé›¶è¾¹æ•° (>0.1): {(internal_edges > 0.1).sum().item()}")
        logger.info(f"  â”œâ”€ éé›¶è¾¹æ•° (>0.5): {(internal_edges > 0.5).sum().item()}")

        # è®¡ç®—ç†è®ºå€¼ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        expected_edges = (edge_mask > 0).sum().item() * num_subgraphs * B
        logger.info(f"  â”œâ”€ ç†è®ºéé›¶è¾¹æ•° (edge_mask>0 Ã— {num_subgraphs} Ã— {B}): {expected_edges}")

        # ========================================================================
        # âœ… ç¬¬2æ­¥ï¼šæ‰§è¡Œé˜¶æ®µä¸€ï¼ˆæ‰€æœ‰å­å›¾çš„å†…éƒ¨å·ç§¯ï¼‰
        # ========================================================================
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æ‰§è¡Œ GCN Block...")

        xs_stage1 = self.gcn_block(x, internal_edges)

        # ğŸ” è°ƒè¯•ç‚¹4: æ£€æŸ¥GCNè¾“å‡º
        logger.info(f"DEBUG [Fusion {fusion_type}]: GCN è¾“å‡º xs_stage1 ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {xs_stage1.shape}")
        logger.info(f"  â”œâ”€ Mean: {xs_stage1.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {xs_stage1.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {xs_stage1.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {xs_stage1.max().item():.6f}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ NaN: {torch.isnan(xs_stage1).any().item()}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ Inf: {torch.isinf(xs_stage1).any().item()}")

        # ========================================================================
        # âœ… ç¬¬3æ­¥ï¼šå‡†å¤‡æ›¿æ¢æ•°æ®
        # ========================================================================

        # 1. åˆ†ç¦» Anchor å’Œ Negatives
        x_anchor_s1 = xs_stage1[:, :P, :]  # Anchorç‰¹å¾ [B, P, d]
        x_negs_s1 = xs_stage1[:, P:, :]  # æ‰€æœ‰è´Ÿæ ·æœ¬ç‰¹å¾ [B, N*P, d]

        # 2. é‡å¡‘å¹¶èšåˆè´Ÿæ ·æœ¬
        x_negs_s1_reshaped = x_negs_s1.reshape(B, num_neg_samples, P, d) # [B, N, P, d]

        # 3. è®¡ç®—ç”¨äºæ›¿æ¢çš„"èšåˆä¿¡æ¯" (å–å¹³å‡)
        x_replacement_info = torch.mean(x_negs_s1_reshaped, dim=1) # [B, P, d]

        # ğŸ” è°ƒè¯•ç‚¹5: æ£€æŸ¥æ›¿æ¢ç‰¹å¾
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æ›¿æ¢ç‰¹å¾ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ x_anchor_s1 Mean: {x_anchor_s1.mean().item():.6f}, Std: {x_anchor_s1.std().item():.6f}")
        logger.info(f"  â”œâ”€ x_replacement_info Mean: {x_replacement_info.mean().item():.6f}, Std: {x_replacement_info.std().item():.6f}")
        logger.info(f"  â”œâ”€ x_replacement_info Min: {x_replacement_info.min().item():.6f}, Max: {x_replacement_info.max().item():.6f}")

        # ========================================================================
        # âœ… ç¬¬4æ­¥ï¼šæ‰§è¡Œ"æ¡ä»¶æ›¿æ¢"
        # ========================================================================

        # å‡†å¤‡æ©ç 
        mask_intrinsic = node_mask.reshape(1, P, 1).to(x.device) # å› æœèŠ‚ç‚¹ [1, P, 1]
        mask_spurious = (1 - mask_intrinsic)                     # è™šå‡èŠ‚ç‚¹ [1, P, 1]

        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æ©ç ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ node_mask shape: {node_mask.shape}")
        logger.info(f"  â”œâ”€ mask_intrinsic éé›¶æ•°: {(mask_intrinsic > 0).sum().item()}")
        logger.info(f"  â”œâ”€ mask_spurious éé›¶æ•°: {(mask_spurious > 0).sum().item()}")

        if fusion_type == "intrinsic":
            # å†…åœ¨èåˆ: Anchorçš„å› æœéƒ¨åˆ†è¢«æ›¿æ¢ï¼Œè™šå‡éƒ¨åˆ†ä¿ç•™
            x_anchor_new = (x_replacement_info * mask_intrinsic) + \
                           (x_anchor_s1 * mask_spurious)
            logger.info(f"DEBUG [Fusion {fusion_type}]: æ‰§è¡Œå†…åœ¨èåˆï¼ˆæ›¿æ¢å› æœèŠ‚ç‚¹ï¼‰")

        elif fusion_type == "spurious":
            # è™šå‡èåˆ: Anchorçš„è™šå‡éƒ¨åˆ†è¢«æ›¿æ¢ï¼Œå› æœéƒ¨åˆ†ä¿ç•™
            x_anchor_new = (x_anchor_s1 * mask_intrinsic) + \
                           (x_replacement_info * mask_spurious)
            logger.info(f"DEBUG [Fusion {fusion_type}]: æ‰§è¡Œè™šå‡èåˆï¼ˆæ›¿æ¢è™šå‡èŠ‚ç‚¹ï¼‰")
        else:
            raise ValueError(f"æœªçŸ¥çš„ fusion_type: {fusion_type}")

        # ğŸ” è°ƒè¯•ç‚¹6: æ£€æŸ¥æ›¿æ¢åçš„ç‰¹å¾
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æ›¿æ¢å x_anchor_new ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {x_anchor_new.shape}")
        logger.info(f"  â”œâ”€ Mean: {x_anchor_new.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {x_anchor_new.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {x_anchor_new.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {x_anchor_new.max().item():.6f}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ NaN: {torch.isnan(x_anchor_new).any().item()}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ Inf: {torch.isinf(x_anchor_new).any().item()}")

        # ========================================================================
        # âœ… ç¬¬5æ­¥ï¼šä½¿ç”¨è¢«æ›¿æ¢åçš„Anchorå›¾è¿›è¡Œé¢„æµ‹
        # ========================================================================
        graph = readout(x_anchor_new) # x_anchor_new æ˜¯ [B, P, d]

        # ğŸ” è°ƒè¯•ç‚¹7: æ£€æŸ¥ readout åçš„ç‰¹å¾
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: Readout åçš„ graph ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {graph.shape}")
        logger.info(f"  â”œâ”€ Mean: {graph.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {graph.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {graph.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {graph.max().item():.6f}")

        logits = self.mlp_causal(graph)

        # ğŸ” è°ƒè¯•ç‚¹8: æ£€æŸ¥æœ€ç»ˆçš„ logits
        logger.info(f"")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æœ€ç»ˆ logits ç»Ÿè®¡:")
        logger.info(f"  â”œâ”€ Shape: {logits.shape}")
        logger.info(f"  â”œâ”€ Mean: {logits.mean().item():.6f}")
        logger.info(f"  â”œâ”€ Std: {logits.std().item():.6f}")
        logger.info(f"  â”œâ”€ Min: {logits.min().item():.6f}")
        logger.info(f"  â”œâ”€ Max: {logits.max().item():.6f}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ NaN: {torch.isnan(logits).any().item()}")
        logger.info(f"  â”œâ”€ æ˜¯å¦æœ‰ Inf: {torch.isinf(logits).any().item()}")

        logger.info(f"{'='*80}")
        logger.info(f"DEBUG [Fusion {fusion_type}]: æ›¿æ¢å¼èåˆç»“æŸ")
        logger.info(f"{'='*80}")
        logger.info(f"")

        return logits
''' 
    
    def prediction_spurious_fusion(
        self,
        x: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        ã€é‡æ„ã€‘
        Spurious Fusion Graph (æ›¿æ¢å¼) - æµ‹è¯•Invariance
        
        ä½¿ç”¨è´Ÿæ ·æœ¬çš„ *è™šå‡èŠ‚ç‚¹* ç‰¹å¾ï¼Œæ›¿æ¢Anchorçš„ *è™šå‡èŠ‚ç‚¹* ç‰¹å¾ã€‚
        """
        return self._perform_fusion_replacement(
            x,
            edge_prior_mask,
            masks,
            fusion_type="spurious",
            is_large_graph=is_large_graph
        )

    def prediction_intrinsic_fusion(
        self,
        x: torch.Tensor,
        edge_prior_mask: torch.Tensor,
        masks: Tuple[torch.Tensor, torch.Tensor],
        is_large_graph: bool = True
    ) -> torch.Tensor:
        """
        ã€é‡æ„ã€‘
        Intrinsic Fusion Graph (æ›¿æ¢å¼) - æµ‹è¯•Sensitivity
        
        ä½¿ç”¨è´Ÿæ ·æœ¬çš„ *å› æœèŠ‚ç‚¹* ç‰¹å¾ï¼Œæ›¿æ¢Anchorçš„ *å› æœèŠ‚ç‚¹* ç‰¹å¾ã€‚
        """
        return self._perform_fusion_replacement(
            x,
            edge_prior_mask,
            masks,
            fusion_type="intrinsic",
            is_large_graph=is_large_graph
        )