"""
ç»Ÿä¸€è®­ç»ƒå…¥å£
ä¼˜é›…çš„ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
"""

import os
import sys
import yaml
import argparse
import logging
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.checkpoint import CheckpointManager
from utils.logging_config import setup_logging
from core.densenet_manager import DenseNetManager
from core.graph_builder import GraphBuilder
from core.trainer import CausalTrainer
from data.dataset import PatchDataset, get_fold_splits
from torch.utils.data import DataLoader, Subset
from data.dataset import collate_fn


logger = logging.getLogger(__name__)


def load_config(config_path: str) -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def setup_experiment(config: dict) -> Path:
    """è®¾ç½®å®éªŒç›®å½•"""
    exp_dir = Path(f"./experiments/{config['exp_name']}")
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    (exp_dir / 'checkpoints').mkdir(exist_ok=True)
    (exp_dir / 'logs').mkdir(exist_ok=True)
    (exp_dir / 'results').mkdir(exist_ok=True)
    
    return exp_dir


def run_training_for_fold(
    config: dict,
    fold: int,
    densenet_model,
    edge_prior_mask,
    checkpoint_manager: CheckpointManager,
    exp_dir: Path
) -> dict:
    """
    è¿è¡Œå•ä¸ª fold çš„è®­ç»ƒ
    
    Args:
        config: é…ç½®å­—å…¸
        fold: fold ç´¢å¼•
        densenet_model: é¢„è®­ç»ƒçš„ DenseNet æ¨¡å‹
        edge_matrix: å›¾è¾¹çŸ©é˜µ
        checkpoint_manager: ç¼“å­˜ç®¡ç†å™¨
        exp_dir: å®éªŒç›®å½•
        
    Returns:
        è®­ç»ƒç»“æœå­—å…¸
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ“‚ Fold {fold}/{config['data']['num_folds']-1}")
    logger.info(f"{'='*80}\n")
    
    # 1. å‡†å¤‡æ•°æ®
    data_dir = os.path.join(config['data']['data_dir'], config['data']['data_name'])
    dataset = PatchDataset(data_dir)
    
    # è·å–æ•°æ®åˆ†å‰²
    train_indices, val_indices, test_indices = get_fold_splits(
        data_dir,
        fold,
        config['split_seed'],
        num_folds=config['data']['num_folds']
    )
    
    logger.info(f"ğŸ“Š Data split:")
    logger.info(f"   Train: {len(train_indices)} samples")
    logger.info(f"   Val:   {len(val_indices)} samples")
    logger.info(f"   Test:  {len(test_indices)} samples")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        Subset(dataset, train_indices),
        batch_size=config['train']['batch_size'],
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=config['densenet']['pretrain']['num_workers'],  # ğŸ”§ ä½¿ç”¨é…ç½®çš„å€¼
        pin_memory=True,       # âœ… ä¿æŒ
        persistent_workers=True,  # ğŸ”§ æ–°å¢ï¼šé¿å…æ¯epoché‡å¯worker
        prefetch_factor=4      # ğŸ”§ æ–°å¢ï¼šé¢„åŠ è½½4ä¸ªbatch
    )

    val_loader = DataLoader(
        Subset(dataset, val_indices),
        batch_size=config['train']['test_batch_size'],
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=config['densenet']['pretrain']['num_workers'],
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )

    test_loader = DataLoader(
        Subset(dataset, test_indices),
        batch_size=config['train']['test_batch_size'],
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=config['densenet']['pretrain']['num_workers'],
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=4
    )
    
    # 2. åˆ›å»ºè®­ç»ƒå™¨
    logger.info("\n" + "-"*80)
    logger.info("ğŸ”§ Initializing trainer...")
    logger.info("-"*80)
    
    work_dir = exp_dir / f"fold_{fold}"
    work_dir.mkdir(exist_ok=True)
    
    trainer = CausalTrainer(
        config=config,
        fold=fold,
        densenet_model=densenet_model,
        edge_prior_mask=edge_prior_mask,
        checkpoint_manager=checkpoint_manager,
        work_dir=str(work_dir),
        device='cuda',
        rank=0
    )
    
    logger.info("âœ… Trainer initialized")
    
    # 3. å¼€å§‹è®­ç»ƒ
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ Starting training...")
    logger.info("="*80)
    
    results = trainer.train(train_loader, val_loader, test_loader)
    
    logger.info(f"\nâœ… Fold {fold} training completed!")
    logger.info(f"   Val Acc:  {results['val_acc']:.4f}")
    logger.info(f"   Test Acc: {results['test_acc']:.4f}")
    
    return results


def main():
    """ä¸»å‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒç³»ç»Ÿ')
    parser.add_argument(
        '--config',
        type=str,
        default='config/train_config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--folds',
        type=int,
        nargs='+',
        default=None,
        help='è¦è®­ç»ƒçš„ fold åˆ—è¡¨ï¼ˆå¦‚ï¼š--folds 0 1 2ï¼‰'
    )
    args = parser.parse_args()
    
    # 1. åŠ è½½é…ç½®
    print("="*80)
    print("ğŸ“‹ åŠ è½½é…ç½®æ–‡ä»¶...")
    print("="*80)
    
    config = load_config(args.config)
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"å®éªŒåç§°: {config['exp_name']}")
    
    # 2. è®¾ç½®å®éªŒç›®å½•å’Œæ—¥å¿—
    exp_dir = setup_experiment(config)
    log_file = exp_dir / 'logs' / f"train_{config['exp_name']}.log"
    setup_logging(log_file=str(log_file), level='INFO')
    
    logger.info("="*80)
    logger.info("ğŸ¯ ä¼˜é›…çš„å› æœå›¾ç¥ç»ç½‘ç»œè®­ç»ƒç³»ç»Ÿ")
    logger.info("="*80)
    logger.info(f"å®éªŒç›®å½•: {exp_dir}")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    
    # 3. åˆå§‹åŒ–ç®¡ç†å™¨
    logger.info("\n" + "="*80)
    logger.info("ğŸ”§ åˆå§‹åŒ–ç®¡ç†å™¨...")
    logger.info("="*80)
    
    checkpoint_manager = CheckpointManager(
        base_cache_dir=config['cache']['base_dir'],
        auto_clean=config['cache'].get('auto_clean', False)
    )
    logger.info(f"âœ… CheckpointManager åˆå§‹åŒ–å®Œæˆ")
    
    densenet_manager = DenseNetManager(
        data_dir=os.path.join(config['data']['data_dir'], config['data']['data_name']),
        checkpoint_manager=checkpoint_manager,
        config=config['densenet']
    )
    logger.info(f"âœ… DenseNetManager åˆå§‹åŒ–å®Œæˆ")
    
    graph_builder = GraphBuilder(
        data_dir=os.path.join(config['data']['data_dir'], config['data']['data_name']),
        checkpoint_manager=checkpoint_manager,
        densenet_manager=densenet_manager,
        config=config['graph']['build']
    )
    logger.info(f"âœ… GraphBuilder åˆå§‹åŒ–å®Œæˆ")
    
    # 4. æ„å»ºå…¨å±€è¾¹çŸ©é˜µï¼ˆæ‰€æœ‰ fold å…±äº«ï¼‰
    logger.info("\n" + "="*80)
    logger.info("ğŸ•¸ï¸  æ„å»º/åŠ è½½è¾¹å…ˆéªŒå€™é€‰é›†...")
    logger.info("="*80)
    
    edge_prior_mask = graph_builder.get_edge_prior_mask(
        split_seed=config['split_seed'],
        fold_for_feature_extraction=0
    )
    logger.info(f"âœ… è¾¹å…ˆéªŒå€™é€‰é›†å‡†å¤‡å®Œæˆ:")
    logger.info(f"   å½¢çŠ¶: {edge_prior_mask.shape}")
    logger.info(f"   å¯å­¦ä¹ è¾¹ä½ç½®æ•°: {edge_prior_mask.sum()}")
    logger.info(f"   å¯†åº¦: {edge_prior_mask.sum() / (edge_prior_mask.shape[0] ** 2):.4f}")
    
    # 5. æ‰§è¡Œæ¯ä¸ª fold çš„è®­ç»ƒ
    folds_to_train = args.folds if args.folds else config['train']['folds']
    all_results = []
    
    logger.info("\n" + "="*80)
    logger.info(f"ğŸš‚ å¼€å§‹è®­ç»ƒ {len(folds_to_train)} ä¸ª fold...")
    logger.info("="*80)
    
    for fold in folds_to_train:
        # è·å–è¯¥ fold çš„é¢„è®­ç»ƒ DenseNet
        logger.info(f"\nğŸ“¦ è·å– Fold {fold} çš„é¢„è®­ç»ƒ DenseNet...")
        densenet_model = densenet_manager.get_pretrained_model(
            fold=fold,
            split_seed=config['split_seed']
        )
        logger.info(f"âœ… DenseNet æ¨¡å‹å‡†å¤‡å®Œæˆ")
        
        # æ‰§è¡Œè®­ç»ƒ
        fold_results = run_training_for_fold(
            config=config,
            fold=fold,
            densenet_model=densenet_model,
            edge_prior_mask=edge_prior_mask,
            checkpoint_manager=checkpoint_manager,
            exp_dir=exp_dir
        )
        
        all_results.append(fold_results)
    
    # 6. æ±‡æ€»ç»“æœ
    logger.info("\n" + "="*80)
    logger.info("ğŸ“Š è®­ç»ƒå®Œæˆï¼æ±‡æ€»ç»“æœï¼š")
    logger.info("="*80)
    
    import numpy as np
    
    val_accs = [r['val_acc'] for r in all_results]
    test_accs = [r['test_acc'] for r in all_results]
    
    logger.info(f"\nğŸ“ˆ éªŒè¯é›†å‡†ç¡®ç‡:")
    for i, acc in enumerate(val_accs):
        logger.info(f"   Fold {folds_to_train[i]}: {acc:.4f}")
    logger.info(f"   å¹³å‡: {np.mean(val_accs):.4f} Â± {np.std(val_accs):.4f}")
    
    logger.info(f"\nğŸ“ˆ æµ‹è¯•é›†å‡†ç¡®ç‡:")
    for i, acc in enumerate(test_accs):
        logger.info(f"   Fold {folds_to_train[i]}: {acc:.4f}")
    logger.info(f"   å¹³å‡: {np.mean(test_accs):.4f} Â± {np.std(test_accs):.4f}")
    
    # 7. ä¿å­˜ç»“æœ
    import json
    results_file = exp_dir / 'results' / 'final_results.json'
    with open(results_file, 'w') as f:
        json.dump({
            'config': config,
            'fold_results': all_results,
            'summary': {
                'val_acc_mean': float(np.mean(val_accs)),
                'val_acc_std': float(np.std(val_accs)),
                'test_acc_mean': float(np.mean(test_accs)),
                'test_acc_std': float(np.std(test_accs))
            }
        }, f, indent=2)
    
    logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ä¼˜é›…åœ°ç»“æŸï¼")
    logger.info("="*80)

if __name__ == "__main__":
    main()